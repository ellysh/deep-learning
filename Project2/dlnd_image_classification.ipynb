{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 4:\n",
      "Image - Min Value: 0 Max Value: 254\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0ffa86d3c8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 4\n",
    "\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    return np.array(x / 255.)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return np.eye(10)[x]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=[None, image_shape[0], \n",
    "                                             image_shape[1], image_shape[2]],\n",
    "                          name=\"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=[None, n_classes],\n",
    "                          name=\"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement Function\n",
    "    input_depth = x_tensor.shape[3].value\n",
    "    filter_weights = tf.Variable(tf.truncated_normal((conv_ksize[0],\n",
    "                                                      conv_ksize[1], input_depth,\n",
    "                                                      conv_num_outputs), stddev=0.1)) # (height, width, input_depth, output_depth)\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    strides = [1, conv_strides[0], conv_strides[1], 1] # (batch, height, width, depth)\n",
    "    padding = 'SAME'\n",
    "\n",
    "    conv = tf.nn.conv2d(x_tensor, filter_weights, strides, padding) + bias\n",
    "    conv = tf.nn.bias_add(conv, bias)\n",
    "    conv = tf.nn.relu(conv)\n",
    "\n",
    "    conv = tf.nn.max_pool(conv, ksize=[1, pool_ksize[0], pool_ksize[1], 1],\n",
    "                          strides=[1, pool_strides[0], pool_strides[1], 1],\n",
    "                          padding=padding)\n",
    "\n",
    "    return conv\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    _, a, b, c = x_tensor.get_shape().as_list()\n",
    "    return tf.reshape(x_tensor, [-1, a * b * c])\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs, activation_fn=tf.nn.relu)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weight = tf.Variable(tf.random_normal([x_tensor.shape[1].value,\n",
    "                                           num_outputs]))\n",
    "    bias = tf.Variable(tf.random_normal([num_outputs]))\n",
    "    \n",
    "    return tf.add(tf.matmul(x_tensor, weight), bias)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    conv = conv2d_maxpool(x, 40, (2,2), (4,4), (2,2), (2,2))\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    conv = flatten(conv)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    conv = fully_conn(conv, 256)\n",
    "    \n",
    "    conv = tf.nn.dropout(conv, keep_prob)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    conv = output(conv, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return conv\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer,\n",
    "                feed_dict={keep_prob: keep_probability,\n",
    "                           x: feature_batch, y: label_batch})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # Calculate Training and Validation accuracy\n",
    "    val_accuracy = session.run(accuracy, feed_dict={\n",
    "           x: valid_features,\n",
    "           y: valid_labels,\n",
    "           keep_prob: 1.0})\n",
    "\n",
    "    val_loss = session.run(cost, feed_dict={\n",
    "           x: valid_features,\n",
    "           y: valid_labels,\n",
    "           keep_prob: 1.0})\n",
    "\n",
    "    print('Loss at {}'.format(val_loss), 'Validation Accuracy at {}'.format(val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 256\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss at 1.9480534791946411 Validation Accuracy at 0.3012000024318695\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss at 1.8031938076019287 Validation Accuracy at 0.36899998784065247\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss at 1.7020282745361328 Validation Accuracy at 0.4007999897003174\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss at 1.6260117292404175 Validation Accuracy at 0.42879998683929443\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss at 1.5896015167236328 Validation Accuracy at 0.4413999915122986\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss at 1.5593818426132202 Validation Accuracy at 0.45899999141693115\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss at 1.538211703300476 Validation Accuracy at 0.4625999927520752\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss at 1.5154778957366943 Validation Accuracy at 0.46860000491142273\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss at 1.4912837743759155 Validation Accuracy at 0.4758000075817108\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss at 1.481188416481018 Validation Accuracy at 0.4797999858856201\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss at 1.4682855606079102 Validation Accuracy at 0.4846000075340271\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss at 1.4542802572250366 Validation Accuracy at 0.4896000027656555\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss at 1.4442178010940552 Validation Accuracy at 0.49140000343322754\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss at 1.4447404146194458 Validation Accuracy at 0.4986000061035156\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss at 1.4281572103500366 Validation Accuracy at 0.49880000948905945\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss at 1.4251290559768677 Validation Accuracy at 0.5\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss at 1.416749358177185 Validation Accuracy at 0.5026000142097473\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss at 1.4132734537124634 Validation Accuracy at 0.5034000277519226\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss at 1.406543254852295 Validation Accuracy at 0.504800021648407\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss at 1.4040921926498413 Validation Accuracy at 0.5095999836921692\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss at 1.3997939825057983 Validation Accuracy at 0.5085999965667725\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss at 1.3962689638137817 Validation Accuracy at 0.5054000020027161\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss at 1.3907667398452759 Validation Accuracy at 0.5105999708175659\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss at 1.3925604820251465 Validation Accuracy at 0.5091999769210815\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss at 1.3931525945663452 Validation Accuracy at 0.5117999911308289\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss at 1.391087293624878 Validation Accuracy at 0.5139999985694885\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss at 1.3902599811553955 Validation Accuracy at 0.5099999904632568\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss at 1.391751766204834 Validation Accuracy at 0.5095999836921692\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss at 1.3790971040725708 Validation Accuracy at 0.5157999992370605\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss at 1.3824763298034668 Validation Accuracy at 0.5152000188827515\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss at 1.3869426250457764 Validation Accuracy at 0.5131999850273132\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss at 1.3752728700637817 Validation Accuracy at 0.5185999870300293\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss at 1.3768812417984009 Validation Accuracy at 0.522599995136261\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss at 1.3749116659164429 Validation Accuracy at 0.520799994468689\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss at 1.3857812881469727 Validation Accuracy at 0.5153999924659729\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss at 1.374514102935791 Validation Accuracy at 0.5220000147819519\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss at 1.3838114738464355 Validation Accuracy at 0.5152000188827515\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss at 1.3811980485916138 Validation Accuracy at 0.5157999992370605\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss at 1.3840566873550415 Validation Accuracy at 0.5131999850273132\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss at 1.3725124597549438 Validation Accuracy at 0.5185999870300293\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss at 1.383637547492981 Validation Accuracy at 0.51419997215271\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss at 1.3710662126541138 Validation Accuracy at 0.5206000208854675\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss at 1.3766204118728638 Validation Accuracy at 0.5171999931335449\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss at 1.3941172361373901 Validation Accuracy at 0.5202000141143799\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss at 1.3847270011901855 Validation Accuracy at 0.5174000263214111\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss at 1.3896751403808594 Validation Accuracy at 0.521399974822998\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss at 1.3817033767700195 Validation Accuracy at 0.5180000066757202\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss at 1.3862015008926392 Validation Accuracy at 0.5199999809265137\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss at 1.3925296068191528 Validation Accuracy at 0.519599974155426\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss at 1.38797128200531 Validation Accuracy at 0.5206000208854675\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss at 1.3884197473526 Validation Accuracy at 0.519599974155426\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss at 1.3852179050445557 Validation Accuracy at 0.5181999802589417\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss at 1.385549783706665 Validation Accuracy at 0.517799973487854\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss at 1.3834654092788696 Validation Accuracy at 0.519599974155426\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss at 1.4190601110458374 Validation Accuracy at 0.5144000053405762\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss at 1.3866792917251587 Validation Accuracy at 0.5228000283241272\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss at 1.3872679471969604 Validation Accuracy at 0.5181999802589417\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss at 1.3881407976150513 Validation Accuracy at 0.5189999938011169\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss at 1.4014167785644531 Validation Accuracy at 0.5174000263214111\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss at 1.38035249710083 Validation Accuracy at 0.520799994468689\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss at 1.390514612197876 Validation Accuracy at 0.5217999815940857\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss at 1.3797657489776611 Validation Accuracy at 0.5246000289916992\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss at 1.3930397033691406 Validation Accuracy at 0.5212000012397766\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss at 1.3904081583023071 Validation Accuracy at 0.5202000141143799\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss at 1.3863418102264404 Validation Accuracy at 0.5221999883651733\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss at 1.3962527513504028 Validation Accuracy at 0.5217999815940857\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss at 1.394636869430542 Validation Accuracy at 0.5252000093460083\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss at 1.3815661668777466 Validation Accuracy at 0.525600016117096\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss at 1.3836085796356201 Validation Accuracy at 0.5284000039100647\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss at 1.381581425666809 Validation Accuracy at 0.5325999855995178\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss at 1.3853912353515625 Validation Accuracy at 0.5296000242233276\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss at 1.391671895980835 Validation Accuracy at 0.5271999835968018\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss at 1.3956271409988403 Validation Accuracy at 0.527400016784668\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss at 1.398362159729004 Validation Accuracy at 0.5246000289916992\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss at 1.385379433631897 Validation Accuracy at 0.5275999903678894\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss at 1.3766844272613525 Validation Accuracy at 0.5293999910354614\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss at 1.3806339502334595 Validation Accuracy at 0.5306000113487244\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss at 1.3825191259384155 Validation Accuracy at 0.5284000039100647\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss at 1.3945125341415405 Validation Accuracy at 0.5281999707221985\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss at 1.3883970975875854 Validation Accuracy at 0.5289999842643738\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss at 1.413244366645813 Validation Accuracy at 0.527999997138977\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss at 1.394432544708252 Validation Accuracy at 0.5271999835968018\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss at 1.3886985778808594 Validation Accuracy at 0.5285999774932861\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss at 1.3961633443832397 Validation Accuracy at 0.525600016117096\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss at 1.3855242729187012 Validation Accuracy at 0.5299999713897705\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss at 1.3959596157073975 Validation Accuracy at 0.5302000045776367\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss at 1.3898680210113525 Validation Accuracy at 0.5382000207901001\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss at 1.4013749361038208 Validation Accuracy at 0.527400016784668\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss at 1.3997986316680908 Validation Accuracy at 0.5289999842643738\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss at 1.3980305194854736 Validation Accuracy at 0.5320000052452087\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss at 1.4084933996200562 Validation Accuracy at 0.5311999917030334\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss at 1.4123095273971558 Validation Accuracy at 0.5333999991416931\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss at 1.4034693241119385 Validation Accuracy at 0.5299999713897705\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss at 1.402529001235962 Validation Accuracy at 0.5270000100135803\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss at 1.4140913486480713 Validation Accuracy at 0.5315999984741211\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss at 1.4276982545852661 Validation Accuracy at 0.5239999890327454\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss at 1.4210023880004883 Validation Accuracy at 0.5297999978065491\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss at 1.4062632322311401 Validation Accuracy at 0.5281999707221985\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss at 2.430445909500122 Validation Accuracy at 0.13420000672340393\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss at 2.2923026084899902 Validation Accuracy at 0.16660000383853912\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss at 2.2074646949768066 Validation Accuracy at 0.1850000023841858\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss at 2.244741439819336 Validation Accuracy at 0.16840000450611115\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss at 2.1269593238830566 Validation Accuracy at 0.22139999270439148\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss at 2.096754550933838 Validation Accuracy at 0.21580000221729279\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss at 2.1049015522003174 Validation Accuracy at 0.23319999873638153\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss at 2.081571578979492 Validation Accuracy at 0.23839999735355377\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss at 2.147773504257202 Validation Accuracy at 0.19939999282360077\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss at 2.063612222671509 Validation Accuracy at 0.25\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss at 2.0551507472991943 Validation Accuracy at 0.2468000054359436\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss at 1.9424222707748413 Validation Accuracy at 0.2768000066280365\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss at 1.9519116878509521 Validation Accuracy at 0.2606000006198883\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss at 2.0322580337524414 Validation Accuracy at 0.2386000007390976\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss at 1.9675577878952026 Validation Accuracy at 0.2702000141143799\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss at 1.936004638671875 Validation Accuracy at 0.2818000018596649\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss at 1.890070915222168 Validation Accuracy at 0.29679998755455017\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss at 1.9316922426223755 Validation Accuracy at 0.27880001068115234\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss at 1.9384129047393799 Validation Accuracy at 0.2741999924182892\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss at 1.9346694946289062 Validation Accuracy at 0.2754000127315521\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss at 1.8785560131072998 Validation Accuracy at 0.2980000078678131\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss at 1.8331800699234009 Validation Accuracy at 0.326200008392334\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss at 1.8895455598831177 Validation Accuracy at 0.2980000078678131\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss at 1.865740180015564 Validation Accuracy at 0.3140000104904175\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss at 1.8837640285491943 Validation Accuracy at 0.3057999908924103\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss at 1.820310354232788 Validation Accuracy at 0.3255999982357025\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss at 1.817404866218567 Validation Accuracy at 0.326200008392334\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss at 1.8168113231658936 Validation Accuracy at 0.3330000042915344\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss at 1.8369841575622559 Validation Accuracy at 0.33340001106262207\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss at 1.8325097560882568 Validation Accuracy at 0.33379998803138733\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss at 1.828312873840332 Validation Accuracy at 0.328000009059906\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss at 1.7779150009155273 Validation Accuracy at 0.3472000062465668\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss at 1.7945786714553833 Validation Accuracy at 0.36160001158714294\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss at 1.8278312683105469 Validation Accuracy at 0.3431999981403351\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss at 1.8054132461547852 Validation Accuracy at 0.35839998722076416\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss at 1.7970916032791138 Validation Accuracy at 0.35740000009536743\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss at 1.7772165536880493 Validation Accuracy at 0.3614000082015991\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss at 1.8075964450836182 Validation Accuracy at 0.3580000102519989\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss at 1.776075005531311 Validation Accuracy at 0.3734000027179718\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss at 1.7690976858139038 Validation Accuracy at 0.38519999384880066\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss at 1.805661916732788 Validation Accuracy at 0.3625999987125397\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss at 1.7405800819396973 Validation Accuracy at 0.3874000012874603\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss at 1.745282769203186 Validation Accuracy at 0.39480000734329224\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss at 1.7542296648025513 Validation Accuracy at 0.3968000113964081\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss at 1.7800480127334595 Validation Accuracy at 0.37040001153945923\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss at 1.7493412494659424 Validation Accuracy at 0.37380000948905945\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss at 1.7474784851074219 Validation Accuracy at 0.39899998903274536\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss at 1.7463746070861816 Validation Accuracy at 0.3959999978542328\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss at 1.766298770904541 Validation Accuracy at 0.3862000107765198\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss at 1.7515279054641724 Validation Accuracy at 0.3991999924182892\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss at 1.7289317846298218 Validation Accuracy at 0.39579999446868896\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss at 1.741493582725525 Validation Accuracy at 0.3991999924182892\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss at 1.7466330528259277 Validation Accuracy at 0.3944000005722046\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss at 1.7218775749206543 Validation Accuracy at 0.40619999170303345\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss at 1.736159324645996 Validation Accuracy at 0.4032000005245209\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss at 1.7397295236587524 Validation Accuracy at 0.3984000086784363\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss at 1.725725769996643 Validation Accuracy at 0.40939998626708984\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss at 1.7180918455123901 Validation Accuracy at 0.41019999980926514\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss at 1.706344485282898 Validation Accuracy at 0.4027999937534332\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss at 1.7388609647750854 Validation Accuracy at 0.39160001277923584\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss at 1.7526179552078247 Validation Accuracy at 0.3898000121116638\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss at 1.6936763525009155 Validation Accuracy at 0.4180000126361847\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss at 1.7107573747634888 Validation Accuracy at 0.39820000529289246\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss at 1.7375617027282715 Validation Accuracy at 0.3901999890804291\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss at 1.7151637077331543 Validation Accuracy at 0.3962000012397766\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss at 1.7049882411956787 Validation Accuracy at 0.4068000018596649\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss at 1.6742427349090576 Validation Accuracy at 0.42320001125335693\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss at 1.6985728740692139 Validation Accuracy at 0.4059999883174896\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss at 1.7005661725997925 Validation Accuracy at 0.4113999903202057\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss at 1.677712082862854 Validation Accuracy at 0.4147999882698059\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss at 1.6875085830688477 Validation Accuracy at 0.4075999855995178\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss at 1.6900453567504883 Validation Accuracy at 0.4124000072479248\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss at 1.6734148263931274 Validation Accuracy at 0.4174000024795532\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss at 1.6717802286148071 Validation Accuracy at 0.41119998693466187\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss at 1.6674308776855469 Validation Accuracy at 0.4235999882221222\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss at 1.6812492609024048 Validation Accuracy at 0.40639999508857727\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss at 1.6715474128723145 Validation Accuracy at 0.4131999909877777\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss at 1.701251745223999 Validation Accuracy at 0.41339999437332153\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss at 1.6933813095092773 Validation Accuracy at 0.40959998965263367\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss at 1.6923445463180542 Validation Accuracy at 0.41200000047683716\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss at 1.6594816446304321 Validation Accuracy at 0.41519999504089355\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss at 1.6834105253219604 Validation Accuracy at 0.41040000319480896\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss at 1.6802024841308594 Validation Accuracy at 0.4002000093460083\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss at 1.6536587476730347 Validation Accuracy at 0.42179998755455017\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss at 1.6389566659927368 Validation Accuracy at 0.41839998960494995\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss at 1.6741422414779663 Validation Accuracy at 0.4027999937534332\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss at 1.668319582939148 Validation Accuracy at 0.42100000381469727\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss at 1.6639105081558228 Validation Accuracy at 0.42340001463890076\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss at 1.6316452026367188 Validation Accuracy at 0.4189999997615814\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss at 1.6577050685882568 Validation Accuracy at 0.40880000591278076\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss at 1.6903517246246338 Validation Accuracy at 0.40560001134872437\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss at 1.6583702564239502 Validation Accuracy at 0.40939998626708984\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss at 1.6342628002166748 Validation Accuracy at 0.4212000072002411\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss at 1.6245414018630981 Validation Accuracy at 0.4205999970436096\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss at 1.6264978647232056 Validation Accuracy at 0.4269999861717224\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss at 1.6458646059036255 Validation Accuracy at 0.41260001063346863\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss at 1.6496562957763672 Validation Accuracy at 0.4146000146865845\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss at 1.6406464576721191 Validation Accuracy at 0.4350000023841858\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss at 1.6123219728469849 Validation Accuracy at 0.4253999888896942\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss at 1.6416003704071045 Validation Accuracy at 0.4050000011920929\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss at 1.6186636686325073 Validation Accuracy at 0.4185999929904938\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss at 1.648505687713623 Validation Accuracy at 0.42239999771118164\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss at 1.6103242635726929 Validation Accuracy at 0.44020000100135803\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss at 1.610310435295105 Validation Accuracy at 0.4449999928474426\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss at 1.630963921546936 Validation Accuracy at 0.412200003862381\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss at 1.6088581085205078 Validation Accuracy at 0.4300000071525574\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss at 1.6130414009094238 Validation Accuracy at 0.4323999881744385\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss at 1.6271663904190063 Validation Accuracy at 0.4214000105857849\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss at 1.597330927848816 Validation Accuracy at 0.4343999922275543\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss at 1.6098620891571045 Validation Accuracy at 0.4228000044822693\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss at 1.620516061782837 Validation Accuracy at 0.423799991607666\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss at 1.6096993684768677 Validation Accuracy at 0.4156000018119812\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss at 1.5957543849945068 Validation Accuracy at 0.4449999928474426\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss at 1.700492024421692 Validation Accuracy at 0.38920000195503235\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss at 1.583241581916809 Validation Accuracy at 0.4458000063896179\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss at 1.5841927528381348 Validation Accuracy at 0.43639999628067017\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss at 1.6023814678192139 Validation Accuracy at 0.446399986743927\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss at 1.5850231647491455 Validation Accuracy at 0.45579999685287476\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss at 1.5569331645965576 Validation Accuracy at 0.4620000123977661\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss at 1.5584560632705688 Validation Accuracy at 0.43939998745918274\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss at 1.560959815979004 Validation Accuracy at 0.45579999685287476\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss at 1.584170937538147 Validation Accuracy at 0.45399999618530273\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss at 1.562540054321289 Validation Accuracy at 0.46059998869895935\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss at 1.5348268747329712 Validation Accuracy at 0.4675999879837036\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss at 1.562247633934021 Validation Accuracy at 0.45260000228881836\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss at 1.5467607975006104 Validation Accuracy at 0.46779999136924744\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss at 1.534244179725647 Validation Accuracy at 0.46480000019073486\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss at 1.5472290515899658 Validation Accuracy at 0.46160000562667847\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss at 1.5277985334396362 Validation Accuracy at 0.4758000075817108\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss at 1.5339184999465942 Validation Accuracy at 0.4553999900817871\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss at 1.5434507131576538 Validation Accuracy at 0.4699999988079071\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss at 1.5391802787780762 Validation Accuracy at 0.4691999852657318\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss at 1.549665093421936 Validation Accuracy at 0.45719999074935913\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss at 1.508636713027954 Validation Accuracy at 0.46959999203681946\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss at 1.511006474494934 Validation Accuracy at 0.4702000021934509\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss at 1.502668023109436 Validation Accuracy at 0.4758000075817108\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss at 1.52267587184906 Validation Accuracy at 0.4797999858856201\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss at 1.4989593029022217 Validation Accuracy at 0.48159998655319214\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss at 1.492219090461731 Validation Accuracy at 0.48019999265670776\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss at 1.510947823524475 Validation Accuracy at 0.46799999475479126\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss at 1.5269168615341187 Validation Accuracy at 0.4620000123977661\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss at 1.5032085180282593 Validation Accuracy at 0.48420000076293945\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss at 1.50608229637146 Validation Accuracy at 0.4797999858856201\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss at 1.494768500328064 Validation Accuracy at 0.4844000041484833\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss at 1.490801453590393 Validation Accuracy at 0.4765999913215637\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss at 1.4902597665786743 Validation Accuracy at 0.48500001430511475\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss at 1.4895447492599487 Validation Accuracy at 0.4830000102519989\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss at 1.4839506149291992 Validation Accuracy at 0.4936000108718872\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss at 1.45991849899292 Validation Accuracy at 0.4909999966621399\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss at 1.4827430248260498 Validation Accuracy at 0.48159998655319214\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss at 1.5032240152359009 Validation Accuracy at 0.4722000062465668\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss at 1.4752185344696045 Validation Accuracy at 0.4918000102043152\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss at 1.4793343544006348 Validation Accuracy at 0.4893999993801117\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss at 1.463022232055664 Validation Accuracy at 0.4950000047683716\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss at 1.4636859893798828 Validation Accuracy at 0.4867999851703644\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss at 1.4724560976028442 Validation Accuracy at 0.4878000020980835\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss at 1.4654041528701782 Validation Accuracy at 0.4943999946117401\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss at 1.461673378944397 Validation Accuracy at 0.4970000088214874\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss at 1.4468013048171997 Validation Accuracy at 0.49799999594688416\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss at 1.4578074216842651 Validation Accuracy at 0.48820000886917114\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss at 1.4531980752944946 Validation Accuracy at 0.5004000067710876\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss at 1.459486961364746 Validation Accuracy at 0.5027999877929688\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss at 1.4671677350997925 Validation Accuracy at 0.49480000138282776\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss at 1.446851372718811 Validation Accuracy at 0.501800000667572\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss at 1.4464305639266968 Validation Accuracy at 0.4957999885082245\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss at 1.4595811367034912 Validation Accuracy at 0.4927999973297119\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss at 1.4510719776153564 Validation Accuracy at 0.49720001220703125\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss at 1.446564793586731 Validation Accuracy at 0.5031999945640564\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss at 1.4380115270614624 Validation Accuracy at 0.5013999938964844\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss at 1.4368951320648193 Validation Accuracy at 0.4959999918937683\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss at 1.437121868133545 Validation Accuracy at 0.503000020980835\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss at 1.4305362701416016 Validation Accuracy at 0.508400022983551\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss at 1.437504768371582 Validation Accuracy at 0.5080000162124634\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss at 1.4234623908996582 Validation Accuracy at 0.5009999871253967\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss at 1.4168075323104858 Validation Accuracy at 0.5045999884605408\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss at 1.4365108013153076 Validation Accuracy at 0.49559998512268066\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss at 1.422781229019165 Validation Accuracy at 0.5026000142097473\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss at 1.4108227491378784 Validation Accuracy at 0.5121999979019165\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss at 1.4210909605026245 Validation Accuracy at 0.4975999891757965\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss at 1.4158626794815063 Validation Accuracy at 0.5019999742507935\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss at 1.4261598587036133 Validation Accuracy at 0.5072000026702881\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss at 1.3919804096221924 Validation Accuracy at 0.5149999856948853\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss at 1.4065923690795898 Validation Accuracy at 0.5077999830245972\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss at 1.398359775543213 Validation Accuracy at 0.5126000046730042\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss at 1.3945114612579346 Validation Accuracy at 0.503600001335144\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss at 1.4038785696029663 Validation Accuracy at 0.5131999850273132\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss at 1.3915961980819702 Validation Accuracy at 0.520799994468689\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss at 1.4084280729293823 Validation Accuracy at 0.5156000256538391\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss at 1.39633047580719 Validation Accuracy at 0.5171999931335449\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss at 1.3893256187438965 Validation Accuracy at 0.5131999850273132\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss at 1.4007693529129028 Validation Accuracy at 0.5144000053405762\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss at 1.391821265220642 Validation Accuracy at 0.5184000134468079\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss at 1.3938071727752686 Validation Accuracy at 0.5228000283241272\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss at 1.3828511238098145 Validation Accuracy at 0.5202000141143799\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss at 1.3760746717453003 Validation Accuracy at 0.5175999999046326\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss at 1.3980202674865723 Validation Accuracy at 0.5171999931335449\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss at 1.3855689764022827 Validation Accuracy at 0.5202000141143799\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss at 1.3822927474975586 Validation Accuracy at 0.5266000032424927\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss at 1.3872621059417725 Validation Accuracy at 0.5194000005722046\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss at 1.3810968399047852 Validation Accuracy at 0.5157999992370605\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss at 1.3883355855941772 Validation Accuracy at 0.5167999863624573\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss at 1.3760955333709717 Validation Accuracy at 0.5246000289916992\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss at 1.3720976114273071 Validation Accuracy at 0.524399995803833\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss at 1.3893530368804932 Validation Accuracy at 0.5181999802589417\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss at 1.388144850730896 Validation Accuracy at 0.5126000046730042\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss at 1.3791993856430054 Validation Accuracy at 0.5192000269889832\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss at 1.368096113204956 Validation Accuracy at 0.5264000296592712\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss at 1.366538166999817 Validation Accuracy at 0.5278000235557556\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss at 1.3636951446533203 Validation Accuracy at 0.525600016117096\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss at 1.3696072101593018 Validation Accuracy at 0.5185999870300293\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss at 1.3772475719451904 Validation Accuracy at 0.5171999931335449\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss at 1.366424798965454 Validation Accuracy at 0.5275999903678894\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss at 1.3647425174713135 Validation Accuracy at 0.5306000113487244\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss at 1.3721507787704468 Validation Accuracy at 0.5248000025749207\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss at 1.3595941066741943 Validation Accuracy at 0.5248000025749207\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss at 1.3832683563232422 Validation Accuracy at 0.5153999924659729\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss at 1.3646953105926514 Validation Accuracy at 0.5293999910354614\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss at 1.3561874628067017 Validation Accuracy at 0.5321999788284302\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss at 1.3654016256332397 Validation Accuracy at 0.525600016117096\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss at 1.359847068786621 Validation Accuracy at 0.5332000255584717\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss at 1.3628531694412231 Validation Accuracy at 0.5248000025749207\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss at 1.359310269355774 Validation Accuracy at 0.5299999713897705\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss at 1.3552578687667847 Validation Accuracy at 0.532800018787384\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss at 1.369640588760376 Validation Accuracy at 0.5260000228881836\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss at 1.3591594696044922 Validation Accuracy at 0.5238000154495239\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss at 1.3618301153182983 Validation Accuracy at 0.5238000154495239\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss at 1.36390221118927 Validation Accuracy at 0.5293999910354614\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss at 1.3497246503829956 Validation Accuracy at 0.534600019454956\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss at 1.3595163822174072 Validation Accuracy at 0.527999997138977\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss at 1.3435380458831787 Validation Accuracy at 0.5325999855995178\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss at 1.3638259172439575 Validation Accuracy at 0.5311999917030334\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss at 1.3684359788894653 Validation Accuracy at 0.5335999727249146\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss at 1.3463722467422485 Validation Accuracy at 0.5404000282287598\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss at 1.3451670408248901 Validation Accuracy at 0.5356000065803528\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss at 1.3524123430252075 Validation Accuracy at 0.5293999910354614\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss at 1.3521007299423218 Validation Accuracy at 0.5270000100135803\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss at 1.3457388877868652 Validation Accuracy at 0.5411999821662903\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss at 1.3437561988830566 Validation Accuracy at 0.5339999794960022\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss at 1.3467364311218262 Validation Accuracy at 0.5428000092506409\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss at 1.3471205234527588 Validation Accuracy at 0.5260000228881836\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss at 1.3558833599090576 Validation Accuracy at 0.5285999774932861\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss at 1.3351776599884033 Validation Accuracy at 0.5379999876022339\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss at 1.347822904586792 Validation Accuracy at 0.5396000146865845\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss at 1.3417701721191406 Validation Accuracy at 0.5392000079154968\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss at 1.3428690433502197 Validation Accuracy at 0.5324000120162964\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss at 1.3423587083816528 Validation Accuracy at 0.5350000262260437\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss at 1.3332085609436035 Validation Accuracy at 0.5388000011444092\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss at 1.3398312330245972 Validation Accuracy at 0.5392000079154968\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss at 1.3268485069274902 Validation Accuracy at 0.5396000146865845\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss at 1.3388782739639282 Validation Accuracy at 0.5374000072479248\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss at 1.350372314453125 Validation Accuracy at 0.531000018119812\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss at 1.3382259607315063 Validation Accuracy at 0.5422000288963318\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss at 1.3436778783798218 Validation Accuracy at 0.5450000166893005\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss at 1.346777081489563 Validation Accuracy at 0.5343999862670898\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss at 1.3515266180038452 Validation Accuracy at 0.5242000222206116\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss at 1.363983154296875 Validation Accuracy at 0.5270000100135803\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss at 1.3427525758743286 Validation Accuracy at 0.5443999767303467\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss at 1.329403042793274 Validation Accuracy at 0.5428000092506409\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss at 1.340668797492981 Validation Accuracy at 0.5361999869346619\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss at 1.3385263681411743 Validation Accuracy at 0.5350000262260437\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss at 1.338724136352539 Validation Accuracy at 0.5360000133514404\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss at 1.3346034288406372 Validation Accuracy at 0.5379999876022339\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss at 1.3309320211410522 Validation Accuracy at 0.5454000234603882\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss at 1.3354014158248901 Validation Accuracy at 0.5388000011444092\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss at 1.3287049531936646 Validation Accuracy at 0.5370000004768372\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss at 1.3369274139404297 Validation Accuracy at 0.5371999740600586\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss at 1.3323777914047241 Validation Accuracy at 0.5401999950408936\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss at 1.3240482807159424 Validation Accuracy at 0.5425999760627747\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss at 1.3328229188919067 Validation Accuracy at 0.5396000146865845\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss at 1.3289519548416138 Validation Accuracy at 0.5404000282287598\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss at 1.3370413780212402 Validation Accuracy at 0.5339999794960022\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss at 1.3211627006530762 Validation Accuracy at 0.5514000058174133\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss at 1.3282088041305542 Validation Accuracy at 0.545199990272522\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss at 1.325773000717163 Validation Accuracy at 0.5375999808311462\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss at 1.335281491279602 Validation Accuracy at 0.5306000113487244\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss at 1.3349695205688477 Validation Accuracy at 0.5315999984741211\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss at 1.323365569114685 Validation Accuracy at 0.5432000160217285\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss at 1.3195966482162476 Validation Accuracy at 0.5450000166893005\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss at 1.3194823265075684 Validation Accuracy at 0.5411999821662903\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss at 1.3162577152252197 Validation Accuracy at 0.5382000207901001\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss at 1.3317209482192993 Validation Accuracy at 0.5361999869346619\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss at 1.3114593029022217 Validation Accuracy at 0.5483999848365784\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss at 1.3171260356903076 Validation Accuracy at 0.5486000180244446\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss at 1.3205456733703613 Validation Accuracy at 0.5473999977111816\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss at 1.315673828125 Validation Accuracy at 0.5425999760627747\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss at 1.3243889808654785 Validation Accuracy at 0.5350000262260437\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss at 1.3121248483657837 Validation Accuracy at 0.5461999773979187\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss at 1.3155781030654907 Validation Accuracy at 0.5442000031471252\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss at 1.3142755031585693 Validation Accuracy at 0.5450000166893005\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss at 1.3129172325134277 Validation Accuracy at 0.5464000105857849\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss at 1.3162134885787964 Validation Accuracy at 0.5436000227928162\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss at 1.3095375299453735 Validation Accuracy at 0.5473999977111816\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss at 1.3083322048187256 Validation Accuracy at 0.5450000166893005\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss at 1.311087727546692 Validation Accuracy at 0.5419999957084656\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss at 1.3170853853225708 Validation Accuracy at 0.5388000011444092\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss at 1.3152235746383667 Validation Accuracy at 0.5425999760627747\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss at 1.3096222877502441 Validation Accuracy at 0.5465999841690063\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss at 1.3106894493103027 Validation Accuracy at 0.5514000058174133\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss at 1.3156675100326538 Validation Accuracy at 0.5424000024795532\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss at 1.3122446537017822 Validation Accuracy at 0.5461999773979187\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss at 1.3127604722976685 Validation Accuracy at 0.5446000099182129\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss at 1.3074504137039185 Validation Accuracy at 0.5446000099182129\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss at 1.3082765340805054 Validation Accuracy at 0.5515999794006348\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss at 1.3155416250228882 Validation Accuracy at 0.5404000282287598\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss at 1.3150806427001953 Validation Accuracy at 0.5379999876022339\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss at 1.310001015663147 Validation Accuracy at 0.545199990272522\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss at 1.3023195266723633 Validation Accuracy at 0.546999990940094\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss at 1.305651307106018 Validation Accuracy at 0.5519999861717224\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss at 1.3139474391937256 Validation Accuracy at 0.546999990940094\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss at 1.3113659620285034 Validation Accuracy at 0.5401999950408936\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss at 1.3170132637023926 Validation Accuracy at 0.5382000207901001\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss at 1.3080209493637085 Validation Accuracy at 0.5446000099182129\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss at 1.3088037967681885 Validation Accuracy at 0.5425999760627747\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss at 1.3080182075500488 Validation Accuracy at 0.5406000018119812\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss at 1.3155747652053833 Validation Accuracy at 0.5371999740600586\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss at 1.314292550086975 Validation Accuracy at 0.5415999889373779\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss at 1.3020769357681274 Validation Accuracy at 0.548799991607666\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss at 1.3047239780426025 Validation Accuracy at 0.5501999855041504\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss at 1.319892168045044 Validation Accuracy at 0.5436000227928162\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss at 1.3158180713653564 Validation Accuracy at 0.5437999963760376\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss at 1.3096297979354858 Validation Accuracy at 0.5447999835014343\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss at 1.3045648336410522 Validation Accuracy at 0.5475999712944031\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss at 1.3024699687957764 Validation Accuracy at 0.5564000010490417\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss at 1.3096959590911865 Validation Accuracy at 0.5419999957084656\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss at 1.3057880401611328 Validation Accuracy at 0.5432000160217285\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss at 1.3052693605422974 Validation Accuracy at 0.5479999780654907\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss at 1.3050471544265747 Validation Accuracy at 0.5508000254631042\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss at 1.298816204071045 Validation Accuracy at 0.550599992275238\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss at 1.3079962730407715 Validation Accuracy at 0.5460000038146973\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss at 1.308963656425476 Validation Accuracy at 0.5401999950408936\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss at 1.3161100149154663 Validation Accuracy at 0.5406000018119812\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss at 1.2975738048553467 Validation Accuracy at 0.5540000200271606\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss at 1.2995078563690186 Validation Accuracy at 0.5523999929428101\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss at 1.3016639947891235 Validation Accuracy at 0.5508000254631042\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss at 1.304997444152832 Validation Accuracy at 0.5490000247955322\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss at 1.3100372552871704 Validation Accuracy at 0.5428000092506409\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss at 1.3028607368469238 Validation Accuracy at 0.5514000058174133\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss at 1.300445556640625 Validation Accuracy at 0.5582000017166138\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss at 1.3042784929275513 Validation Accuracy at 0.5537999868392944\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss at 1.3027055263519287 Validation Accuracy at 0.545199990272522\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss at 1.308307409286499 Validation Accuracy at 0.5455999970436096\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss at 1.297061800956726 Validation Accuracy at 0.5529999732971191\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss at 1.303526759147644 Validation Accuracy at 0.5515999794006348\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss at 1.3015508651733398 Validation Accuracy at 0.550599992275238\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss at 1.3091660737991333 Validation Accuracy at 0.5419999957084656\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss at 1.3004558086395264 Validation Accuracy at 0.5508000254631042\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss at 1.298531413078308 Validation Accuracy at 0.5541999936103821\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss at 1.3020398616790771 Validation Accuracy at 0.5551999807357788\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss at 1.3008010387420654 Validation Accuracy at 0.5501999855041504\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss at 1.2998266220092773 Validation Accuracy at 0.545799970626831\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss at 1.3051997423171997 Validation Accuracy at 0.5479999780654907\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss at 1.2910382747650146 Validation Accuracy at 0.5491999983787537\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss at 1.298130989074707 Validation Accuracy at 0.5511999726295471\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss at 1.3044365644454956 Validation Accuracy at 0.548799991607666\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss at 1.3076128959655762 Validation Accuracy at 0.5428000092506409\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss at 1.2983474731445312 Validation Accuracy at 0.5523999929428101\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss at 1.2964632511138916 Validation Accuracy at 0.5529999732971191\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss at 1.2901650667190552 Validation Accuracy at 0.5527999997138977\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss at 1.3079307079315186 Validation Accuracy at 0.5461999773979187\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss at 1.299078345298767 Validation Accuracy at 0.5473999977111816\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss at 1.3128935098648071 Validation Accuracy at 0.54339998960495\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss at 1.3000108003616333 Validation Accuracy at 0.5508000254631042\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss at 1.2914601564407349 Validation Accuracy at 0.5601999759674072\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss at 1.304297924041748 Validation Accuracy at 0.5454000234603882\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss at 1.2934908866882324 Validation Accuracy at 0.5519999861717224\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss at 1.3050662279129028 Validation Accuracy at 0.5493999719619751\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss at 1.2951805591583252 Validation Accuracy at 0.557200014591217\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss at 1.2906708717346191 Validation Accuracy at 0.5601999759674072\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss at 1.3011537790298462 Validation Accuracy at 0.5515999794006348\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss at 1.3015954494476318 Validation Accuracy at 0.550000011920929\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss at 1.3013745546340942 Validation Accuracy at 0.553600013256073\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss at 1.3002731800079346 Validation Accuracy at 0.550599992275238\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss at 1.2897554636001587 Validation Accuracy at 0.5590000152587891\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss at 1.3034584522247314 Validation Accuracy at 0.5455999970436096\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss at 1.2974872589111328 Validation Accuracy at 0.5522000193595886\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss at 1.30288565158844 Validation Accuracy at 0.5478000044822693\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss at 1.2931396961212158 Validation Accuracy at 0.553600013256073\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss at 1.302133560180664 Validation Accuracy at 0.5527999997138977\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss at 1.2942230701446533 Validation Accuracy at 0.5555999875068665\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss at 1.293625831604004 Validation Accuracy at 0.5522000193595886\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss at 1.2961349487304688 Validation Accuracy at 0.5537999868392944\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss at 1.2937747240066528 Validation Accuracy at 0.5523999929428101\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss at 1.2998995780944824 Validation Accuracy at 0.5616000294685364\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss at 1.3027331829071045 Validation Accuracy at 0.5532000064849854\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss at 1.311313271522522 Validation Accuracy at 0.5432000160217285\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss at 1.295042634010315 Validation Accuracy at 0.5559999942779541\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss at 1.2962154150009155 Validation Accuracy at 0.5514000058174133\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss at 1.2965190410614014 Validation Accuracy at 0.5576000213623047\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss at 1.2959387302398682 Validation Accuracy at 0.5514000058174133\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss at 1.3051282167434692 Validation Accuracy at 0.555400013923645\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss at 1.3048502206802368 Validation Accuracy at 0.545799970626831\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss at 1.2884917259216309 Validation Accuracy at 0.5526000261306763\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss at 1.2918636798858643 Validation Accuracy at 0.557200014591217\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss at 1.3001437187194824 Validation Accuracy at 0.5454000234603882\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss at 1.3065603971481323 Validation Accuracy at 0.5440000295639038\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss at 1.291141152381897 Validation Accuracy at 0.553600013256073\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss at 1.2875125408172607 Validation Accuracy at 0.551800012588501\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss at 1.2912020683288574 Validation Accuracy at 0.5576000213623047\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss at 1.2996195554733276 Validation Accuracy at 0.5464000105857849\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss at 1.2924467325210571 Validation Accuracy at 0.5508000254631042\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss at 1.297546148300171 Validation Accuracy at 0.5537999868392944\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss at 1.2889530658721924 Validation Accuracy at 0.5573999881744385\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss at 1.2866281270980835 Validation Accuracy at 0.557200014591217\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss at 1.3004132509231567 Validation Accuracy at 0.5519999861717224\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss at 1.2923458814620972 Validation Accuracy at 0.5527999997138977\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss at 1.2939143180847168 Validation Accuracy at 0.5501999855041504\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss at 1.301424264907837 Validation Accuracy at 0.5501999855041504\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss at 1.2911957502365112 Validation Accuracy at 0.5559999942779541\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss at 1.2896274328231812 Validation Accuracy at 0.5613999962806702\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss at 1.2913472652435303 Validation Accuracy at 0.5565999746322632\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss at 1.3013662099838257 Validation Accuracy at 0.5478000044822693\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss at 1.2925621271133423 Validation Accuracy at 0.5618000030517578\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss at 1.2847135066986084 Validation Accuracy at 0.5651999711990356\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss at 1.2993799448013306 Validation Accuracy at 0.5491999983787537\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss at 1.297248363494873 Validation Accuracy at 0.5504000186920166\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss at 1.2888685464859009 Validation Accuracy at 0.5622000098228455\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss at 1.294083833694458 Validation Accuracy at 0.555400013923645\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss at 1.2895596027374268 Validation Accuracy at 0.5586000084877014\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss at 1.291117787361145 Validation Accuracy at 0.5559999942779541\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss at 1.2934907674789429 Validation Accuracy at 0.5526000261306763\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss at 1.29392671585083 Validation Accuracy at 0.5565999746322632\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss at 1.291784644126892 Validation Accuracy at 0.5514000058174133\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss at 1.284796118736267 Validation Accuracy at 0.5648000240325928\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss at 1.3015111684799194 Validation Accuracy at 0.5491999983787537\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss at 1.2914625406265259 Validation Accuracy at 0.5564000010490417\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss at 1.2913724184036255 Validation Accuracy at 0.5537999868392944\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss at 1.2845089435577393 Validation Accuracy at 0.555400013923645\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss at 1.2872090339660645 Validation Accuracy at 0.5609999895095825\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss at 1.3052252531051636 Validation Accuracy at 0.5501999855041504\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss at 1.2872898578643799 Validation Accuracy at 0.5527999997138977\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss at 1.29288911819458 Validation Accuracy at 0.5509999990463257\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss at 1.297365665435791 Validation Accuracy at 0.550000011920929\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss at 1.2858421802520752 Validation Accuracy at 0.5618000030517578\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss at 1.293803334236145 Validation Accuracy at 0.5558000206947327\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss at 1.2924128770828247 Validation Accuracy at 0.5546000003814697\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss at 1.2917208671569824 Validation Accuracy at 0.5562000274658203\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss at 1.2867110967636108 Validation Accuracy at 0.5573999881744385\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss at 1.2886868715286255 Validation Accuracy at 0.5612000226974487\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss at 1.293774127960205 Validation Accuracy at 0.5501999855041504\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss at 1.2865804433822632 Validation Accuracy at 0.557200014591217\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss at 1.287319302558899 Validation Accuracy at 0.5587999820709229\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss at 1.2870217561721802 Validation Accuracy at 0.5547999739646912\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss at 1.2893660068511963 Validation Accuracy at 0.5564000010490417\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss at 1.3027459383010864 Validation Accuracy at 0.5473999977111816\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss at 1.299172282218933 Validation Accuracy at 0.5501999855041504\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss at 1.2913453578948975 Validation Accuracy at 0.5537999868392944\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss at 1.2862465381622314 Validation Accuracy at 0.5598000288009644\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss at 1.2947579622268677 Validation Accuracy at 0.5594000220298767\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss at 1.3001307249069214 Validation Accuracy at 0.5508000254631042\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss at 1.2901867628097534 Validation Accuracy at 0.5565999746322632\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss at 1.2936421632766724 Validation Accuracy at 0.5527999997138977\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss at 1.2890138626098633 Validation Accuracy at 0.5586000084877014\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss at 1.2866121530532837 Validation Accuracy at 0.5586000084877014\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss at 1.2870550155639648 Validation Accuracy at 0.5612000226974487\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss at 1.2885509729385376 Validation Accuracy at 0.5608000159263611\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss at 1.3017845153808594 Validation Accuracy at 0.546999990940094\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss at 1.295424461364746 Validation Accuracy at 0.5540000200271606\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss at 1.2877053022384644 Validation Accuracy at 0.5586000084877014\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss at 1.2916048765182495 Validation Accuracy at 0.5537999868392944\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss at 1.2855116128921509 Validation Accuracy at 0.5550000071525574\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss at 1.292725920677185 Validation Accuracy at 0.5582000017166138\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss at 1.304152011871338 Validation Accuracy at 0.5569999814033508\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss at 1.2844101190567017 Validation Accuracy at 0.5612000226974487\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss at 1.291929841041565 Validation Accuracy at 0.5564000010490417\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss at 1.285368800163269 Validation Accuracy at 0.5630000233650208\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss at 1.2955220937728882 Validation Accuracy at 0.5540000200271606\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss at 1.2858200073242188 Validation Accuracy at 0.5623999834060669\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss at 1.2887476682662964 Validation Accuracy at 0.5583999752998352\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss at 1.2901772260665894 Validation Accuracy at 0.5565999746322632\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss at 1.2900769710540771 Validation Accuracy at 0.5573999881744385\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss at 1.2941288948059082 Validation Accuracy at 0.5547999739646912\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss at 1.2818719148635864 Validation Accuracy at 0.5619999766349792\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss at 1.28446364402771 Validation Accuracy at 0.5623999834060669\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss at 1.298555612564087 Validation Accuracy at 0.5491999983787537\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss at 1.2928415536880493 Validation Accuracy at 0.553600013256073\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss at 1.2840757369995117 Validation Accuracy at 0.5622000098228455\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss at 1.2931156158447266 Validation Accuracy at 0.5541999936103821\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss at 1.284412145614624 Validation Accuracy at 0.5619999766349792\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss at 1.2935727834701538 Validation Accuracy at 0.555400013923645\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss at 1.2985310554504395 Validation Accuracy at 0.550000011920929\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss at 1.2964669466018677 Validation Accuracy at 0.5559999942779541\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss at 1.294692039489746 Validation Accuracy at 0.5540000200271606\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss at 1.2871798276901245 Validation Accuracy at 0.5626000165939331\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss at 1.3011327981948853 Validation Accuracy at 0.551800012588501\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss at 1.2888882160186768 Validation Accuracy at 0.5583999752998352\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss at 1.2976157665252686 Validation Accuracy at 0.5562000274658203\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss at 1.2893072366714478 Validation Accuracy at 0.5613999962806702\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss at 1.2926759719848633 Validation Accuracy at 0.5616000294685364\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss at 1.2977272272109985 Validation Accuracy at 0.5590000152587891\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss at 1.2930684089660645 Validation Accuracy at 0.5514000058174133\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss at 1.2931718826293945 Validation Accuracy at 0.5555999875068665\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss at 1.289607048034668 Validation Accuracy at 0.5613999962806702\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss at 1.2881296873092651 Validation Accuracy at 0.5626000165939331\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss at 1.292039394378662 Validation Accuracy at 0.5587999820709229\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss at 1.2864049673080444 Validation Accuracy at 0.5577999949455261\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss at 1.2898797988891602 Validation Accuracy at 0.5576000213623047\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss at 1.2899699211120605 Validation Accuracy at 0.5616000294685364\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss at 1.29130220413208 Validation Accuracy at 0.5586000084877014\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss at 1.2863318920135498 Validation Accuracy at 0.5583999752998352\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss at 1.2888976335525513 Validation Accuracy at 0.555400013923645\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss at 1.2893928289413452 Validation Accuracy at 0.5532000064849854\n",
      "Epoch 101, CIFAR-10 Batch 2:  Loss at 1.2942415475845337 Validation Accuracy at 0.5547999739646912\n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss at 1.2878623008728027 Validation Accuracy at 0.5669999718666077\n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss at 1.2903363704681396 Validation Accuracy at 0.5582000017166138\n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss at 1.2836073637008667 Validation Accuracy at 0.5630000233650208\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss at 1.3014695644378662 Validation Accuracy at 0.5583999752998352\n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss at 1.2890039682388306 Validation Accuracy at 0.5622000098228455\n",
      "Epoch 102, CIFAR-10 Batch 3:  Loss at 1.2911691665649414 Validation Accuracy at 0.557200014591217\n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss at 1.3013885021209717 Validation Accuracy at 0.5514000058174133\n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss at 1.2933772802352905 Validation Accuracy at 0.5582000017166138\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss at 1.3014466762542725 Validation Accuracy at 0.5546000003814697\n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss at 1.2875261306762695 Validation Accuracy at 0.5676000118255615\n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss at 1.29427170753479 Validation Accuracy at 0.5626000165939331\n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss at 1.2923262119293213 Validation Accuracy at 0.5558000206947327\n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss at 1.2937527894973755 Validation Accuracy at 0.5609999895095825\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss at 1.2976062297821045 Validation Accuracy at 0.5587999820709229\n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss at 1.2856471538543701 Validation Accuracy at 0.5622000098228455\n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss at 1.285735011100769 Validation Accuracy at 0.5587999820709229\n",
      "Epoch 104, CIFAR-10 Batch 4:  Loss at 1.2966488599777222 Validation Accuracy at 0.5537999868392944\n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss at 1.2916840314865112 Validation Accuracy at 0.5631999969482422\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss at 1.2914304733276367 Validation Accuracy at 0.5631999969482422\n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss at 1.2866162061691284 Validation Accuracy at 0.5618000030517578\n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss at 1.2907145023345947 Validation Accuracy at 0.5601999759674072\n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss at 1.292893409729004 Validation Accuracy at 0.5608000159263611\n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss at 1.2900631427764893 Validation Accuracy at 0.557200014591217\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss at 1.2913976907730103 Validation Accuracy at 0.5654000043869019\n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss at 1.2908427715301514 Validation Accuracy at 0.5622000098228455\n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss at 1.2956056594848633 Validation Accuracy at 0.5605999827384949\n",
      "Epoch 106, CIFAR-10 Batch 4:  Loss at 1.298831820487976 Validation Accuracy at 0.5582000017166138\n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss at 1.292818546295166 Validation Accuracy at 0.5598000288009644\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss at 1.2874948978424072 Validation Accuracy at 0.5623999834060669\n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss at 1.301705002784729 Validation Accuracy at 0.5608000159263611\n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss at 1.2936619520187378 Validation Accuracy at 0.5587999820709229\n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss at 1.3047181367874146 Validation Accuracy at 0.555400013923645\n",
      "Epoch 107, CIFAR-10 Batch 5:  Loss at 1.291273593902588 Validation Accuracy at 0.5568000078201294\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss at 1.2902827262878418 Validation Accuracy at 0.5627999901771545\n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss at 1.2857434749603271 Validation Accuracy at 0.5609999895095825\n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss at 1.290330171585083 Validation Accuracy at 0.5630000233650208\n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss at 1.2954649925231934 Validation Accuracy at 0.5573999881744385\n",
      "Epoch 108, CIFAR-10 Batch 5:  Loss at 1.299316167831421 Validation Accuracy at 0.5573999881744385\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss at 1.2992156744003296 Validation Accuracy at 0.5605999827384949\n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss at 1.2888327836990356 Validation Accuracy at 0.5605999827384949\n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss at 1.2913144826889038 Validation Accuracy at 0.5622000098228455\n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss at 1.3032857179641724 Validation Accuracy at 0.5527999997138977\n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss at 1.2938847541809082 Validation Accuracy at 0.5569999814033508\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss at 1.2967387437820435 Validation Accuracy at 0.5547999739646912\n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss at 1.287907600402832 Validation Accuracy at 0.5565999746322632\n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss at 1.3001749515533447 Validation Accuracy at 0.5555999875068665\n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss at 1.3003302812576294 Validation Accuracy at 0.5573999881744385\n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss at 1.299786925315857 Validation Accuracy at 0.5555999875068665\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss at 1.2922192811965942 Validation Accuracy at 0.5644000172615051\n",
      "Epoch 111, CIFAR-10 Batch 2:  Loss at 1.2959730625152588 Validation Accuracy at 0.5527999997138977\n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss at 1.2868373394012451 Validation Accuracy at 0.5600000023841858\n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss at 1.2968412637710571 Validation Accuracy at 0.5591999888420105\n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss at 1.2945127487182617 Validation Accuracy at 0.5540000200271606\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss at 1.301618218421936 Validation Accuracy at 0.5564000010490417\n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss at 1.2934967279434204 Validation Accuracy at 0.5722000002861023\n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss at 1.2981477975845337 Validation Accuracy at 0.5590000152587891\n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss at 1.2951606512069702 Validation Accuracy at 0.5609999895095825\n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss at 1.2920856475830078 Validation Accuracy at 0.5658000111579895\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss at 1.3055518865585327 Validation Accuracy at 0.555400013923645\n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss at 1.2891956567764282 Validation Accuracy at 0.5685999989509583\n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss at 1.2969578504562378 Validation Accuracy at 0.5605999827384949\n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss at 1.2986555099487305 Validation Accuracy at 0.5559999942779541\n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss at 1.3016846179962158 Validation Accuracy at 0.5555999875068665\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss at 1.2991617918014526 Validation Accuracy at 0.5577999949455261\n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss at 1.297817349433899 Validation Accuracy at 0.5641999840736389\n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss at 1.29506516456604 Validation Accuracy at 0.5640000104904175\n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss at 1.3012478351593018 Validation Accuracy at 0.5587999820709229\n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss at 1.298157811164856 Validation Accuracy at 0.5544000267982483\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss at 1.3086241483688354 Validation Accuracy at 0.5564000010490417\n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss at 1.289602518081665 Validation Accuracy at 0.5609999895095825\n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss at 1.2910656929016113 Validation Accuracy at 0.5630000233650208\n",
      "Epoch 115, CIFAR-10 Batch 4:  Loss at 1.3001203536987305 Validation Accuracy at 0.5555999875068665\n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss at 1.2913590669631958 Validation Accuracy at 0.5604000091552734\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss at 1.2967571020126343 Validation Accuracy at 0.5627999901771545\n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss at 1.2961872816085815 Validation Accuracy at 0.5633999705314636\n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss at 1.2965599298477173 Validation Accuracy at 0.5573999881744385\n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss at 1.3047233819961548 Validation Accuracy at 0.5623999834060669\n",
      "Epoch 116, CIFAR-10 Batch 5:  Loss at 1.2998673915863037 Validation Accuracy at 0.5587999820709229\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss at 1.2976189851760864 Validation Accuracy at 0.5626000165939331\n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss at 1.2924143075942993 Validation Accuracy at 0.5630000233650208\n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss at 1.2920739650726318 Validation Accuracy at 0.5604000091552734\n",
      "Epoch 117, CIFAR-10 Batch 4:  Loss at 1.2993172407150269 Validation Accuracy at 0.5587999820709229\n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss at 1.292812705039978 Validation Accuracy at 0.5600000023841858\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss at 1.3022727966308594 Validation Accuracy at 0.5659999847412109\n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss at 1.2952275276184082 Validation Accuracy at 0.5644000172615051\n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss at 1.2949810028076172 Validation Accuracy at 0.5609999895095825\n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss at 1.297486662864685 Validation Accuracy at 0.5623999834060669\n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss at 1.2967664003372192 Validation Accuracy at 0.5604000091552734\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss at 1.303802728652954 Validation Accuracy at 0.5601999759674072\n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss at 1.2920141220092773 Validation Accuracy at 0.5658000111579895\n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss at 1.2915565967559814 Validation Accuracy at 0.5654000043869019\n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss at 1.294626235961914 Validation Accuracy at 0.5649999976158142\n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss at 1.2979336977005005 Validation Accuracy at 0.5636000037193298\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss at 1.307436466217041 Validation Accuracy at 0.5605999827384949\n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss at 1.2985594272613525 Validation Accuracy at 0.5651999711990356\n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss at 1.3001556396484375 Validation Accuracy at 0.5600000023841858\n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss at 1.3053174018859863 Validation Accuracy at 0.5600000023841858\n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss at 1.299926519393921 Validation Accuracy at 0.5605999827384949\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss at 1.29354989528656 Validation Accuracy at 0.5627999901771545\n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss at 1.2890865802764893 Validation Accuracy at 0.5684000253677368\n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss at 1.2982265949249268 Validation Accuracy at 0.5613999962806702\n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss at 1.2981082201004028 Validation Accuracy at 0.5583999752998352\n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss at 1.3010368347167969 Validation Accuracy at 0.5591999888420105\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss at 1.3005789518356323 Validation Accuracy at 0.5601999759674072\n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss at 1.2967662811279297 Validation Accuracy at 0.5626000165939331\n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss at 1.30314302444458 Validation Accuracy at 0.5609999895095825\n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss at 1.300060510635376 Validation Accuracy at 0.5622000098228455\n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss at 1.2979154586791992 Validation Accuracy at 0.5564000010490417\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss at 1.298306941986084 Validation Accuracy at 0.5631999969482422\n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss at 1.3064734935760498 Validation Accuracy at 0.5582000017166138\n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss at 1.3044697046279907 Validation Accuracy at 0.5565999746322632\n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss at 1.3038181066513062 Validation Accuracy at 0.5583999752998352\n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss at 1.302977204322815 Validation Accuracy at 0.5582000017166138\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss at 1.3031367063522339 Validation Accuracy at 0.5618000030517578\n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss at 1.2983423471450806 Validation Accuracy at 0.5623999834060669\n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss at 1.2945834398269653 Validation Accuracy at 0.5608000159263611\n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss at 1.3080365657806396 Validation Accuracy at 0.5555999875068665\n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss at 1.3004767894744873 Validation Accuracy at 0.5601999759674072\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss at 1.3005940914154053 Validation Accuracy at 0.5637999773025513\n",
      "Epoch 125, CIFAR-10 Batch 2:  Loss at 1.2938941717147827 Validation Accuracy at 0.5623999834060669\n",
      "Epoch 125, CIFAR-10 Batch 3:  Loss at 1.2935864925384521 Validation Accuracy at 0.5666000247001648\n",
      "Epoch 125, CIFAR-10 Batch 4:  Loss at 1.308301329612732 Validation Accuracy at 0.5609999895095825\n",
      "Epoch 125, CIFAR-10 Batch 5:  Loss at 1.2971712350845337 Validation Accuracy at 0.5637999773025513\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss at 1.3008564710617065 Validation Accuracy at 0.5641999840736389\n",
      "Epoch 126, CIFAR-10 Batch 2:  Loss at 1.3017529249191284 Validation Accuracy at 0.5587999820709229\n",
      "Epoch 126, CIFAR-10 Batch 3:  Loss at 1.310408592224121 Validation Accuracy at 0.5640000104904175\n",
      "Epoch 126, CIFAR-10 Batch 4:  Loss at 1.3019013404846191 Validation Accuracy at 0.5630000233650208\n",
      "Epoch 126, CIFAR-10 Batch 5:  Loss at 1.2932615280151367 Validation Accuracy at 0.5644000172615051\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss at 1.3065577745437622 Validation Accuracy at 0.5595999956130981\n",
      "Epoch 127, CIFAR-10 Batch 2:  Loss at 1.2998160123825073 Validation Accuracy at 0.5616000294685364\n",
      "Epoch 127, CIFAR-10 Batch 3:  Loss at 1.3011949062347412 Validation Accuracy at 0.5595999956130981\n",
      "Epoch 127, CIFAR-10 Batch 4:  Loss at 1.30836820602417 Validation Accuracy at 0.5644000172615051\n",
      "Epoch 127, CIFAR-10 Batch 5:  Loss at 1.3018370866775513 Validation Accuracy at 0.5541999936103821\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss at 1.3064336776733398 Validation Accuracy at 0.5598000288009644\n",
      "Epoch 128, CIFAR-10 Batch 2:  Loss at 1.3002939224243164 Validation Accuracy at 0.5633999705314636\n",
      "Epoch 128, CIFAR-10 Batch 3:  Loss at 1.304147720336914 Validation Accuracy at 0.5555999875068665\n",
      "Epoch 128, CIFAR-10 Batch 4:  Loss at 1.3061681985855103 Validation Accuracy at 0.5590000152587891\n",
      "Epoch 128, CIFAR-10 Batch 5:  Loss at 1.2981278896331787 Validation Accuracy at 0.5619999766349792\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss at 1.3051691055297852 Validation Accuracy at 0.5623999834060669\n",
      "Epoch 129, CIFAR-10 Batch 2:  Loss at 1.296931266784668 Validation Accuracy at 0.5605999827384949\n",
      "Epoch 129, CIFAR-10 Batch 3:  Loss at 1.3049089908599854 Validation Accuracy at 0.5626000165939331\n",
      "Epoch 129, CIFAR-10 Batch 4:  Loss at 1.320541262626648 Validation Accuracy at 0.5551999807357788\n",
      "Epoch 129, CIFAR-10 Batch 5:  Loss at 1.3057116270065308 Validation Accuracy at 0.5598000288009644\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss at 1.301551342010498 Validation Accuracy at 0.5590000152587891\n",
      "Epoch 130, CIFAR-10 Batch 2:  Loss at 1.3023501634597778 Validation Accuracy at 0.5583999752998352\n",
      "Epoch 130, CIFAR-10 Batch 3:  Loss at 1.302179217338562 Validation Accuracy at 0.5636000037193298\n",
      "Epoch 130, CIFAR-10 Batch 4:  Loss at 1.3086007833480835 Validation Accuracy at 0.5594000220298767\n",
      "Epoch 130, CIFAR-10 Batch 5:  Loss at 1.3051890134811401 Validation Accuracy at 0.5562000274658203\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss at 1.304943561553955 Validation Accuracy at 0.5604000091552734\n",
      "Epoch 131, CIFAR-10 Batch 2:  Loss at 1.318017601966858 Validation Accuracy at 0.5537999868392944\n",
      "Epoch 131, CIFAR-10 Batch 3:  Loss at 1.3063851594924927 Validation Accuracy at 0.5640000104904175\n",
      "Epoch 131, CIFAR-10 Batch 4:  Loss at 1.3105350732803345 Validation Accuracy at 0.5619999766349792\n",
      "Epoch 131, CIFAR-10 Batch 5:  Loss at 1.304518222808838 Validation Accuracy at 0.5619999766349792\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss at 1.3123942613601685 Validation Accuracy at 0.5608000159263611\n",
      "Epoch 132, CIFAR-10 Batch 2:  Loss at 1.298996090888977 Validation Accuracy at 0.5586000084877014\n",
      "Epoch 132, CIFAR-10 Batch 3:  Loss at 1.3110300302505493 Validation Accuracy at 0.5636000037193298\n",
      "Epoch 132, CIFAR-10 Batch 4:  Loss at 1.3067415952682495 Validation Accuracy at 0.5627999901771545\n",
      "Epoch 132, CIFAR-10 Batch 5:  Loss at 1.3007898330688477 Validation Accuracy at 0.5648000240325928\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss at 1.3159689903259277 Validation Accuracy at 0.5600000023841858\n",
      "Epoch 133, CIFAR-10 Batch 2:  Loss at 1.3130927085876465 Validation Accuracy at 0.5604000091552734\n",
      "Epoch 133, CIFAR-10 Batch 3:  Loss at 1.3060028553009033 Validation Accuracy at 0.5655999779701233\n",
      "Epoch 133, CIFAR-10 Batch 4:  Loss at 1.310973882675171 Validation Accuracy at 0.5576000213623047\n",
      "Epoch 133, CIFAR-10 Batch 5:  Loss at 1.3001856803894043 Validation Accuracy at 0.5613999962806702\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss at 1.3107986450195312 Validation Accuracy at 0.5651999711990356\n",
      "Epoch 134, CIFAR-10 Batch 2:  Loss at 1.3032710552215576 Validation Accuracy at 0.5608000159263611\n",
      "Epoch 134, CIFAR-10 Batch 3:  Loss at 1.30036461353302 Validation Accuracy at 0.5655999779701233\n",
      "Epoch 134, CIFAR-10 Batch 4:  Loss at 1.3062806129455566 Validation Accuracy at 0.5582000017166138\n",
      "Epoch 134, CIFAR-10 Batch 5:  Loss at 1.3112516403198242 Validation Accuracy at 0.5605999827384949\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss at 1.32136869430542 Validation Accuracy at 0.5591999888420105\n",
      "Epoch 135, CIFAR-10 Batch 2:  Loss at 1.3134000301361084 Validation Accuracy at 0.5551999807357788\n",
      "Epoch 135, CIFAR-10 Batch 3:  Loss at 1.3109633922576904 Validation Accuracy at 0.5587999820709229\n",
      "Epoch 135, CIFAR-10 Batch 4:  Loss at 1.3136245012283325 Validation Accuracy at 0.5591999888420105\n",
      "Epoch 135, CIFAR-10 Batch 5:  Loss at 1.3033438920974731 Validation Accuracy at 0.5609999895095825\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss at 1.316281795501709 Validation Accuracy at 0.5559999942779541\n",
      "Epoch 136, CIFAR-10 Batch 2:  Loss at 1.3089492321014404 Validation Accuracy at 0.5626000165939331\n",
      "Epoch 136, CIFAR-10 Batch 3:  Loss at 1.3117624521255493 Validation Accuracy at 0.5631999969482422\n",
      "Epoch 136, CIFAR-10 Batch 4:  Loss at 1.3145771026611328 Validation Accuracy at 0.5637999773025513\n",
      "Epoch 136, CIFAR-10 Batch 5:  Loss at 1.3077222108840942 Validation Accuracy at 0.5583999752998352\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss at 1.3092644214630127 Validation Accuracy at 0.5582000017166138\n",
      "Epoch 137, CIFAR-10 Batch 2:  Loss at 1.315402865409851 Validation Accuracy at 0.5580000281333923\n",
      "Epoch 137, CIFAR-10 Batch 3:  Loss at 1.3137704133987427 Validation Accuracy at 0.5613999962806702\n",
      "Epoch 137, CIFAR-10 Batch 4:  Loss at 1.3121753931045532 Validation Accuracy at 0.5636000037193298\n",
      "Epoch 137, CIFAR-10 Batch 5:  Loss at 1.3044838905334473 Validation Accuracy at 0.5573999881744385\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss at 1.3236000537872314 Validation Accuracy at 0.5558000206947327\n",
      "Epoch 138, CIFAR-10 Batch 2:  Loss at 1.3119243383407593 Validation Accuracy at 0.5591999888420105\n",
      "Epoch 138, CIFAR-10 Batch 3:  Loss at 1.3152492046356201 Validation Accuracy at 0.5582000017166138\n",
      "Epoch 138, CIFAR-10 Batch 4:  Loss at 1.315255880355835 Validation Accuracy at 0.5622000098228455\n",
      "Epoch 138, CIFAR-10 Batch 5:  Loss at 1.3120756149291992 Validation Accuracy at 0.5582000017166138\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss at 1.3193331956863403 Validation Accuracy at 0.5591999888420105\n",
      "Epoch 139, CIFAR-10 Batch 2:  Loss at 1.312119960784912 Validation Accuracy at 0.5623999834060669\n",
      "Epoch 139, CIFAR-10 Batch 3:  Loss at 1.3130900859832764 Validation Accuracy at 0.5619999766349792\n",
      "Epoch 139, CIFAR-10 Batch 4:  Loss at 1.3170750141143799 Validation Accuracy at 0.5631999969482422\n",
      "Epoch 139, CIFAR-10 Batch 5:  Loss at 1.3101881742477417 Validation Accuracy at 0.5633999705314636\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss at 1.310665249824524 Validation Accuracy at 0.5622000098228455\n",
      "Epoch 140, CIFAR-10 Batch 2:  Loss at 1.3066855669021606 Validation Accuracy at 0.5667999982833862\n",
      "Epoch 140, CIFAR-10 Batch 3:  Loss at 1.3155181407928467 Validation Accuracy at 0.5582000017166138\n",
      "Epoch 140, CIFAR-10 Batch 4:  Loss at 1.3244258165359497 Validation Accuracy at 0.5604000091552734\n",
      "Epoch 140, CIFAR-10 Batch 5:  Loss at 1.3035922050476074 Validation Accuracy at 0.5648000240325928\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss at 1.3154196739196777 Validation Accuracy at 0.5669999718666077\n",
      "Epoch 141, CIFAR-10 Batch 2:  Loss at 1.3057111501693726 Validation Accuracy at 0.5636000037193298\n",
      "Epoch 141, CIFAR-10 Batch 3:  Loss at 1.3205146789550781 Validation Accuracy at 0.5636000037193298\n",
      "Epoch 141, CIFAR-10 Batch 4:  Loss at 1.3213931322097778 Validation Accuracy at 0.5587999820709229\n",
      "Epoch 141, CIFAR-10 Batch 5:  Loss at 1.3068698644638062 Validation Accuracy at 0.5667999982833862\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss at 1.313218116760254 Validation Accuracy at 0.5651999711990356\n",
      "Epoch 142, CIFAR-10 Batch 2:  Loss at 1.3178207874298096 Validation Accuracy at 0.5583999752998352\n",
      "Epoch 142, CIFAR-10 Batch 3:  Loss at 1.3210978507995605 Validation Accuracy at 0.5666000247001648\n",
      "Epoch 142, CIFAR-10 Batch 4:  Loss at 1.324355125427246 Validation Accuracy at 0.5600000023841858\n",
      "Epoch 142, CIFAR-10 Batch 5:  Loss at 1.3065928220748901 Validation Accuracy at 0.5622000098228455\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss at 1.32439386844635 Validation Accuracy at 0.5612000226974487\n",
      "Epoch 143, CIFAR-10 Batch 2:  Loss at 1.3206031322479248 Validation Accuracy at 0.5565999746322632\n",
      "Epoch 143, CIFAR-10 Batch 3:  Loss at 1.3245246410369873 Validation Accuracy at 0.5616000294685364\n",
      "Epoch 143, CIFAR-10 Batch 4:  Loss at 1.318569540977478 Validation Accuracy at 0.5565999746322632\n",
      "Epoch 143, CIFAR-10 Batch 5:  Loss at 1.3135384321212769 Validation Accuracy at 0.5600000023841858\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss at 1.3247414827346802 Validation Accuracy at 0.5608000159263611\n",
      "Epoch 144, CIFAR-10 Batch 2:  Loss at 1.307392954826355 Validation Accuracy at 0.5633999705314636\n",
      "Epoch 144, CIFAR-10 Batch 3:  Loss at 1.3265421390533447 Validation Accuracy at 0.5594000220298767\n",
      "Epoch 144, CIFAR-10 Batch 4:  Loss at 1.320828914642334 Validation Accuracy at 0.5630000233650208\n",
      "Epoch 144, CIFAR-10 Batch 5:  Loss at 1.3138291835784912 Validation Accuracy at 0.5636000037193298\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss at 1.3174651861190796 Validation Accuracy at 0.5627999901771545\n",
      "Epoch 145, CIFAR-10 Batch 2:  Loss at 1.3190587759017944 Validation Accuracy at 0.5537999868392944\n",
      "Epoch 145, CIFAR-10 Batch 3:  Loss at 1.3298693895339966 Validation Accuracy at 0.5623999834060669\n",
      "Epoch 145, CIFAR-10 Batch 4:  Loss at 1.327708125114441 Validation Accuracy at 0.5526000261306763\n",
      "Epoch 145, CIFAR-10 Batch 5:  Loss at 1.311959147453308 Validation Accuracy at 0.5562000274658203\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss at 1.3151829242706299 Validation Accuracy at 0.5605999827384949\n",
      "Epoch 146, CIFAR-10 Batch 2:  Loss at 1.314223051071167 Validation Accuracy at 0.5612000226974487\n",
      "Epoch 146, CIFAR-10 Batch 3:  Loss at 1.3238002061843872 Validation Accuracy at 0.5587999820709229\n",
      "Epoch 146, CIFAR-10 Batch 4:  Loss at 1.313620686531067 Validation Accuracy at 0.5680000185966492\n",
      "Epoch 146, CIFAR-10 Batch 5:  Loss at 1.3215892314910889 Validation Accuracy at 0.5583999752998352\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss at 1.3182125091552734 Validation Accuracy at 0.5640000104904175\n",
      "Epoch 147, CIFAR-10 Batch 2:  Loss at 1.3188581466674805 Validation Accuracy at 0.5627999901771545\n",
      "Epoch 147, CIFAR-10 Batch 3:  Loss at 1.3237298727035522 Validation Accuracy at 0.5627999901771545\n",
      "Epoch 147, CIFAR-10 Batch 4:  Loss at 1.3241177797317505 Validation Accuracy at 0.5604000091552734\n",
      "Epoch 147, CIFAR-10 Batch 5:  Loss at 1.316810131072998 Validation Accuracy at 0.5591999888420105\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss at 1.3167309761047363 Validation Accuracy at 0.5654000043869019\n",
      "Epoch 148, CIFAR-10 Batch 2:  Loss at 1.3156654834747314 Validation Accuracy at 0.5609999895095825\n",
      "Epoch 148, CIFAR-10 Batch 3:  Loss at 1.3238319158554077 Validation Accuracy at 0.5609999895095825\n",
      "Epoch 148, CIFAR-10 Batch 4:  Loss at 1.3306587934494019 Validation Accuracy at 0.5595999956130981\n",
      "Epoch 148, CIFAR-10 Batch 5:  Loss at 1.318914532661438 Validation Accuracy at 0.5613999962806702\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss at 1.3302109241485596 Validation Accuracy at 0.5604000091552734\n",
      "Epoch 149, CIFAR-10 Batch 2:  Loss at 1.3169395923614502 Validation Accuracy at 0.5598000288009644\n",
      "Epoch 149, CIFAR-10 Batch 3:  Loss at 1.3195239305496216 Validation Accuracy at 0.5685999989509583\n",
      "Epoch 149, CIFAR-10 Batch 4:  Loss at 1.326882004737854 Validation Accuracy at 0.5595999956130981\n",
      "Epoch 149, CIFAR-10 Batch 5:  Loss at 1.314172625541687 Validation Accuracy at 0.5583999752998352\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss at 1.3281922340393066 Validation Accuracy at 0.5605999827384949\n",
      "Epoch 150, CIFAR-10 Batch 2:  Loss at 1.3138035535812378 Validation Accuracy at 0.5604000091552734\n",
      "Epoch 150, CIFAR-10 Batch 3:  Loss at 1.321602702140808 Validation Accuracy at 0.5595999956130981\n",
      "Epoch 150, CIFAR-10 Batch 4:  Loss at 1.3272840976715088 Validation Accuracy at 0.5684000253677368\n",
      "Epoch 150, CIFAR-10 Batch 5:  Loss at 1.3212471008300781 Validation Accuracy at 0.5627999901771545\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss at 1.3295247554779053 Validation Accuracy at 0.5613999962806702\n",
      "Epoch 151, CIFAR-10 Batch 2:  Loss at 1.3259326219558716 Validation Accuracy at 0.5546000003814697\n",
      "Epoch 151, CIFAR-10 Batch 3:  Loss at 1.3299428224563599 Validation Accuracy at 0.5568000078201294\n",
      "Epoch 151, CIFAR-10 Batch 4:  Loss at 1.3332451581954956 Validation Accuracy at 0.5582000017166138\n",
      "Epoch 151, CIFAR-10 Batch 5:  Loss at 1.324499487876892 Validation Accuracy at 0.5637999773025513\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss at 1.3236796855926514 Validation Accuracy at 0.5559999942779541\n",
      "Epoch 152, CIFAR-10 Batch 2:  Loss at 1.3222973346710205 Validation Accuracy at 0.5618000030517578\n",
      "Epoch 152, CIFAR-10 Batch 3:  Loss at 1.327789545059204 Validation Accuracy at 0.5627999901771545\n",
      "Epoch 152, CIFAR-10 Batch 4:  Loss at 1.329237461090088 Validation Accuracy at 0.5622000098228455\n",
      "Epoch 152, CIFAR-10 Batch 5:  Loss at 1.3204890489578247 Validation Accuracy at 0.5630000233650208\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss at 1.3370475769042969 Validation Accuracy at 0.5582000017166138\n",
      "Epoch 153, CIFAR-10 Batch 2:  Loss at 1.3228776454925537 Validation Accuracy at 0.5612000226974487\n",
      "Epoch 153, CIFAR-10 Batch 3:  Loss at 1.3395793437957764 Validation Accuracy at 0.5630000233650208\n",
      "Epoch 153, CIFAR-10 Batch 4:  Loss at 1.3318613767623901 Validation Accuracy at 0.5582000017166138\n",
      "Epoch 153, CIFAR-10 Batch 5:  Loss at 1.3224340677261353 Validation Accuracy at 0.5594000220298767\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss at 1.3328231573104858 Validation Accuracy at 0.5618000030517578\n",
      "Epoch 154, CIFAR-10 Batch 2:  Loss at 1.3213021755218506 Validation Accuracy at 0.5586000084877014\n",
      "Epoch 154, CIFAR-10 Batch 3:  Loss at 1.3345259428024292 Validation Accuracy at 0.5573999881744385\n",
      "Epoch 154, CIFAR-10 Batch 4:  Loss at 1.3319402933120728 Validation Accuracy at 0.5601999759674072\n",
      "Epoch 154, CIFAR-10 Batch 5:  Loss at 1.3182162046432495 Validation Accuracy at 0.5605999827384949\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss at 1.324710726737976 Validation Accuracy at 0.5604000091552734\n",
      "Epoch 155, CIFAR-10 Batch 2:  Loss at 1.3212906122207642 Validation Accuracy at 0.5612000226974487\n",
      "Epoch 155, CIFAR-10 Batch 3:  Loss at 1.3275867700576782 Validation Accuracy at 0.5627999901771545\n",
      "Epoch 155, CIFAR-10 Batch 4:  Loss at 1.3456816673278809 Validation Accuracy at 0.5618000030517578\n",
      "Epoch 155, CIFAR-10 Batch 5:  Loss at 1.3289439678192139 Validation Accuracy at 0.5626000165939331\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss at 1.334460735321045 Validation Accuracy at 0.5631999969482422\n",
      "Epoch 156, CIFAR-10 Batch 2:  Loss at 1.3181474208831787 Validation Accuracy at 0.5582000017166138\n",
      "Epoch 156, CIFAR-10 Batch 3:  Loss at 1.329663634300232 Validation Accuracy at 0.5580000281333923\n",
      "Epoch 156, CIFAR-10 Batch 4:  Loss at 1.3370225429534912 Validation Accuracy at 0.5565999746322632\n",
      "Epoch 156, CIFAR-10 Batch 5:  Loss at 1.3243449926376343 Validation Accuracy at 0.5613999962806702\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss at 1.3303534984588623 Validation Accuracy at 0.5630000233650208\n",
      "Epoch 157, CIFAR-10 Batch 2:  Loss at 1.325811505317688 Validation Accuracy at 0.5582000017166138\n",
      "Epoch 157, CIFAR-10 Batch 3:  Loss at 1.3338626623153687 Validation Accuracy at 0.5649999976158142\n",
      "Epoch 157, CIFAR-10 Batch 4:  Loss at 1.3380869626998901 Validation Accuracy at 0.5627999901771545\n",
      "Epoch 157, CIFAR-10 Batch 5:  Loss at 1.3240057229995728 Validation Accuracy at 0.5600000023841858\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss at 1.3371615409851074 Validation Accuracy at 0.5631999969482422\n",
      "Epoch 158, CIFAR-10 Batch 2:  Loss at 1.3253577947616577 Validation Accuracy at 0.5659999847412109\n",
      "Epoch 158, CIFAR-10 Batch 3:  Loss at 1.3405343294143677 Validation Accuracy at 0.5604000091552734\n",
      "Epoch 158, CIFAR-10 Batch 4:  Loss at 1.328953504562378 Validation Accuracy at 0.5637999773025513\n",
      "Epoch 158, CIFAR-10 Batch 5:  Loss at 1.3230836391448975 Validation Accuracy at 0.5619999766349792\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss at 1.3452142477035522 Validation Accuracy at 0.5605999827384949\n",
      "Epoch 159, CIFAR-10 Batch 2:  Loss at 1.3198364973068237 Validation Accuracy at 0.5551999807357788\n",
      "Epoch 159, CIFAR-10 Batch 3:  Loss at 1.3395217657089233 Validation Accuracy at 0.5590000152587891\n",
      "Epoch 159, CIFAR-10 Batch 4:  Loss at 1.3404521942138672 Validation Accuracy at 0.5587999820709229\n",
      "Epoch 159, CIFAR-10 Batch 5:  Loss at 1.326047420501709 Validation Accuracy at 0.5637999773025513\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss at 1.3401211500167847 Validation Accuracy at 0.5577999949455261\n",
      "Epoch 160, CIFAR-10 Batch 2:  Loss at 1.319528579711914 Validation Accuracy at 0.5613999962806702\n",
      "Epoch 160, CIFAR-10 Batch 3:  Loss at 1.3329410552978516 Validation Accuracy at 0.5591999888420105\n",
      "Epoch 160, CIFAR-10 Batch 4:  Loss at 1.347266674041748 Validation Accuracy at 0.5546000003814697\n",
      "Epoch 160, CIFAR-10 Batch 5:  Loss at 1.3328001499176025 Validation Accuracy at 0.5605999827384949\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss at 1.347887396812439 Validation Accuracy at 0.5608000159263611\n",
      "Epoch 161, CIFAR-10 Batch 2:  Loss at 1.3409732580184937 Validation Accuracy at 0.5511999726295471\n",
      "Epoch 161, CIFAR-10 Batch 3:  Loss at 1.3370767831802368 Validation Accuracy at 0.5609999895095825\n",
      "Epoch 161, CIFAR-10 Batch 4:  Loss at 1.343644142150879 Validation Accuracy at 0.5577999949455261\n",
      "Epoch 161, CIFAR-10 Batch 5:  Loss at 1.332981824874878 Validation Accuracy at 0.5590000152587891\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss at 1.336164951324463 Validation Accuracy at 0.5640000104904175\n",
      "Epoch 162, CIFAR-10 Batch 2:  Loss at 1.3403412103652954 Validation Accuracy at 0.5568000078201294\n",
      "Epoch 162, CIFAR-10 Batch 3:  Loss at 1.3364129066467285 Validation Accuracy at 0.5604000091552734\n",
      "Epoch 162, CIFAR-10 Batch 4:  Loss at 1.3456838130950928 Validation Accuracy at 0.557200014591217\n",
      "Epoch 162, CIFAR-10 Batch 5:  Loss at 1.3265491724014282 Validation Accuracy at 0.5569999814033508\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss at 1.3521485328674316 Validation Accuracy at 0.5558000206947327\n",
      "Epoch 163, CIFAR-10 Batch 2:  Loss at 1.3267804384231567 Validation Accuracy at 0.5558000206947327\n",
      "Epoch 163, CIFAR-10 Batch 3:  Loss at 1.3453106880187988 Validation Accuracy at 0.5631999969482422\n",
      "Epoch 163, CIFAR-10 Batch 4:  Loss at 1.3357789516448975 Validation Accuracy at 0.5662000179290771\n",
      "Epoch 163, CIFAR-10 Batch 5:  Loss at 1.3302351236343384 Validation Accuracy at 0.5544000267982483\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss at 1.3370516300201416 Validation Accuracy at 0.5594000220298767\n",
      "Epoch 164, CIFAR-10 Batch 2:  Loss at 1.330909013748169 Validation Accuracy at 0.5564000010490417\n",
      "Epoch 164, CIFAR-10 Batch 3:  Loss at 1.3367328643798828 Validation Accuracy at 0.5618000030517578\n",
      "Epoch 164, CIFAR-10 Batch 4:  Loss at 1.345989465713501 Validation Accuracy at 0.5586000084877014\n",
      "Epoch 164, CIFAR-10 Batch 5:  Loss at 1.3308359384536743 Validation Accuracy at 0.5616000294685364\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss at 1.338563323020935 Validation Accuracy at 0.5594000220298767\n",
      "Epoch 165, CIFAR-10 Batch 2:  Loss at 1.335097312927246 Validation Accuracy at 0.5562000274658203\n",
      "Epoch 165, CIFAR-10 Batch 3:  Loss at 1.3430962562561035 Validation Accuracy at 0.5644000172615051\n",
      "Epoch 165, CIFAR-10 Batch 4:  Loss at 1.3431460857391357 Validation Accuracy at 0.5608000159263611\n",
      "Epoch 165, CIFAR-10 Batch 5:  Loss at 1.339417576789856 Validation Accuracy at 0.5594000220298767\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss at 1.3429445028305054 Validation Accuracy at 0.5565999746322632\n",
      "Epoch 166, CIFAR-10 Batch 2:  Loss at 1.3290612697601318 Validation Accuracy at 0.5608000159263611\n",
      "Epoch 166, CIFAR-10 Batch 3:  Loss at 1.338702917098999 Validation Accuracy at 0.5623999834060669\n",
      "Epoch 166, CIFAR-10 Batch 4:  Loss at 1.3477085828781128 Validation Accuracy at 0.5586000084877014\n",
      "Epoch 166, CIFAR-10 Batch 5:  Loss at 1.3378548622131348 Validation Accuracy at 0.5586000084877014\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss at 1.3492510318756104 Validation Accuracy at 0.5613999962806702\n",
      "Epoch 167, CIFAR-10 Batch 2:  Loss at 1.3273605108261108 Validation Accuracy at 0.5587999820709229\n",
      "Epoch 167, CIFAR-10 Batch 3:  Loss at 1.3463488817214966 Validation Accuracy at 0.5604000091552734\n",
      "Epoch 167, CIFAR-10 Batch 4:  Loss at 1.3415882587432861 Validation Accuracy at 0.5613999962806702\n",
      "Epoch 167, CIFAR-10 Batch 5:  Loss at 1.336120843887329 Validation Accuracy at 0.5583999752998352\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss at 1.3428542613983154 Validation Accuracy at 0.5550000071525574\n",
      "Epoch 168, CIFAR-10 Batch 2:  Loss at 1.3395475149154663 Validation Accuracy at 0.5550000071525574\n",
      "Epoch 168, CIFAR-10 Batch 3:  Loss at 1.3478577136993408 Validation Accuracy at 0.5636000037193298\n",
      "Epoch 168, CIFAR-10 Batch 4:  Loss at 1.3530162572860718 Validation Accuracy at 0.553600013256073\n",
      "Epoch 168, CIFAR-10 Batch 5:  Loss at 1.3390295505523682 Validation Accuracy at 0.5626000165939331\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss at 1.3482303619384766 Validation Accuracy at 0.5595999956130981\n",
      "Epoch 169, CIFAR-10 Batch 2:  Loss at 1.334728717803955 Validation Accuracy at 0.5582000017166138\n",
      "Epoch 169, CIFAR-10 Batch 3:  Loss at 1.3420873880386353 Validation Accuracy at 0.5631999969482422\n",
      "Epoch 169, CIFAR-10 Batch 4:  Loss at 1.352343201637268 Validation Accuracy at 0.5636000037193298\n",
      "Epoch 169, CIFAR-10 Batch 5:  Loss at 1.346616268157959 Validation Accuracy at 0.5577999949455261\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss at 1.3516665697097778 Validation Accuracy at 0.5641999840736389\n",
      "Epoch 170, CIFAR-10 Batch 2:  Loss at 1.3378876447677612 Validation Accuracy at 0.5551999807357788\n",
      "Epoch 170, CIFAR-10 Batch 3:  Loss at 1.3443012237548828 Validation Accuracy at 0.5654000043869019\n",
      "Epoch 170, CIFAR-10 Batch 4:  Loss at 1.3376046419143677 Validation Accuracy at 0.5633999705314636\n",
      "Epoch 170, CIFAR-10 Batch 5:  Loss at 1.3403888940811157 Validation Accuracy at 0.5577999949455261\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss at 1.3469408750534058 Validation Accuracy at 0.5537999868392944\n",
      "Epoch 171, CIFAR-10 Batch 2:  Loss at 1.3472917079925537 Validation Accuracy at 0.555400013923645\n",
      "Epoch 171, CIFAR-10 Batch 3:  Loss at 1.3432111740112305 Validation Accuracy at 0.5613999962806702\n",
      "Epoch 171, CIFAR-10 Batch 4:  Loss at 1.3586900234222412 Validation Accuracy at 0.5586000084877014\n",
      "Epoch 171, CIFAR-10 Batch 5:  Loss at 1.3323047161102295 Validation Accuracy at 0.5637999773025513\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss at 1.348136305809021 Validation Accuracy at 0.5562000274658203\n",
      "Epoch 172, CIFAR-10 Batch 2:  Loss at 1.3504106998443604 Validation Accuracy at 0.5479999780654907\n",
      "Epoch 172, CIFAR-10 Batch 3:  Loss at 1.3425428867340088 Validation Accuracy at 0.5608000159263611\n",
      "Epoch 172, CIFAR-10 Batch 4:  Loss at 1.3412193059921265 Validation Accuracy at 0.5559999942779541\n",
      "Epoch 172, CIFAR-10 Batch 5:  Loss at 1.3441283702850342 Validation Accuracy at 0.5541999936103821\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss at 1.3474491834640503 Validation Accuracy at 0.5604000091552734\n",
      "Epoch 173, CIFAR-10 Batch 2:  Loss at 1.3360697031021118 Validation Accuracy at 0.5565999746322632\n",
      "Epoch 173, CIFAR-10 Batch 3:  Loss at 1.356796145439148 Validation Accuracy at 0.5576000213623047\n",
      "Epoch 173, CIFAR-10 Batch 4:  Loss at 1.342817783355713 Validation Accuracy at 0.5608000159263611\n",
      "Epoch 173, CIFAR-10 Batch 5:  Loss at 1.3394362926483154 Validation Accuracy at 0.5626000165939331\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss at 1.3409680128097534 Validation Accuracy at 0.5544000267982483\n",
      "Epoch 174, CIFAR-10 Batch 2:  Loss at 1.3467177152633667 Validation Accuracy at 0.5511999726295471\n",
      "Epoch 174, CIFAR-10 Batch 3:  Loss at 1.3584831953048706 Validation Accuracy at 0.5618000030517578\n",
      "Epoch 174, CIFAR-10 Batch 4:  Loss at 1.359894037246704 Validation Accuracy at 0.5558000206947327\n",
      "Epoch 174, CIFAR-10 Batch 5:  Loss at 1.3371919393539429 Validation Accuracy at 0.5616000294685364\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss at 1.3519861698150635 Validation Accuracy at 0.5604000091552734\n",
      "Epoch 175, CIFAR-10 Batch 2:  Loss at 1.34346342086792 Validation Accuracy at 0.5544000267982483\n",
      "Epoch 175, CIFAR-10 Batch 3:  Loss at 1.3587132692337036 Validation Accuracy at 0.5613999962806702\n",
      "Epoch 175, CIFAR-10 Batch 4:  Loss at 1.3626128435134888 Validation Accuracy at 0.5591999888420105\n",
      "Epoch 175, CIFAR-10 Batch 5:  Loss at 1.343680500984192 Validation Accuracy at 0.5576000213623047\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss at 1.3485629558563232 Validation Accuracy at 0.5497999787330627\n",
      "Epoch 176, CIFAR-10 Batch 2:  Loss at 1.3408833742141724 Validation Accuracy at 0.551800012588501\n",
      "Epoch 176, CIFAR-10 Batch 3:  Loss at 1.359717607498169 Validation Accuracy at 0.5558000206947327\n",
      "Epoch 176, CIFAR-10 Batch 4:  Loss at 1.3587559461593628 Validation Accuracy at 0.5537999868392944\n",
      "Epoch 176, CIFAR-10 Batch 5:  Loss at 1.354299545288086 Validation Accuracy at 0.5546000003814697\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss at 1.3552227020263672 Validation Accuracy at 0.5582000017166138\n",
      "Epoch 177, CIFAR-10 Batch 2:  Loss at 1.3466787338256836 Validation Accuracy at 0.5544000267982483\n",
      "Epoch 177, CIFAR-10 Batch 3:  Loss at 1.3564099073410034 Validation Accuracy at 0.5586000084877014\n",
      "Epoch 177, CIFAR-10 Batch 4:  Loss at 1.362494945526123 Validation Accuracy at 0.5583999752998352\n",
      "Epoch 177, CIFAR-10 Batch 5:  Loss at 1.3457356691360474 Validation Accuracy at 0.5590000152587891\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss at 1.3456476926803589 Validation Accuracy at 0.5587999820709229\n",
      "Epoch 178, CIFAR-10 Batch 2:  Loss at 1.3520127534866333 Validation Accuracy at 0.5483999848365784\n",
      "Epoch 178, CIFAR-10 Batch 3:  Loss at 1.3626962900161743 Validation Accuracy at 0.5583999752998352\n",
      "Epoch 178, CIFAR-10 Batch 4:  Loss at 1.3597708940505981 Validation Accuracy at 0.5551999807357788\n",
      "Epoch 178, CIFAR-10 Batch 5:  Loss at 1.3557168245315552 Validation Accuracy at 0.5544000267982483\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss at 1.3542293310165405 Validation Accuracy at 0.557200014591217\n",
      "Epoch 179, CIFAR-10 Batch 2:  Loss at 1.344982624053955 Validation Accuracy at 0.5540000200271606\n",
      "Epoch 179, CIFAR-10 Batch 3:  Loss at 1.362471103668213 Validation Accuracy at 0.5551999807357788\n",
      "Epoch 179, CIFAR-10 Batch 4:  Loss at 1.3654879331588745 Validation Accuracy at 0.5564000010490417\n",
      "Epoch 179, CIFAR-10 Batch 5:  Loss at 1.3514775037765503 Validation Accuracy at 0.5547999739646912\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss at 1.3541579246520996 Validation Accuracy at 0.5613999962806702\n",
      "Epoch 180, CIFAR-10 Batch 2:  Loss at 1.3383079767227173 Validation Accuracy at 0.5547999739646912\n",
      "Epoch 180, CIFAR-10 Batch 3:  Loss at 1.3606483936309814 Validation Accuracy at 0.5526000261306763\n",
      "Epoch 180, CIFAR-10 Batch 4:  Loss at 1.361782431602478 Validation Accuracy at 0.5540000200271606\n",
      "Epoch 180, CIFAR-10 Batch 5:  Loss at 1.3510847091674805 Validation Accuracy at 0.5600000023841858\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss at 1.3606488704681396 Validation Accuracy at 0.5601999759674072\n",
      "Epoch 181, CIFAR-10 Batch 2:  Loss at 1.3568583726882935 Validation Accuracy at 0.548799991607666\n",
      "Epoch 181, CIFAR-10 Batch 3:  Loss at 1.3657100200653076 Validation Accuracy at 0.5577999949455261\n",
      "Epoch 181, CIFAR-10 Batch 4:  Loss at 1.3676038980484009 Validation Accuracy at 0.5582000017166138\n",
      "Epoch 181, CIFAR-10 Batch 5:  Loss at 1.362064003944397 Validation Accuracy at 0.5519999861717224\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss at 1.3761937618255615 Validation Accuracy at 0.5546000003814697\n",
      "Epoch 182, CIFAR-10 Batch 2:  Loss at 1.3518520593643188 Validation Accuracy at 0.5515999794006348\n",
      "Epoch 182, CIFAR-10 Batch 3:  Loss at 1.3807330131530762 Validation Accuracy at 0.5562000274658203\n",
      "Epoch 182, CIFAR-10 Batch 4:  Loss at 1.3664597272872925 Validation Accuracy at 0.5604000091552734\n",
      "Epoch 182, CIFAR-10 Batch 5:  Loss at 1.3500558137893677 Validation Accuracy at 0.5551999807357788\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss at 1.3746094703674316 Validation Accuracy at 0.5540000200271606\n",
      "Epoch 183, CIFAR-10 Batch 2:  Loss at 1.3466014862060547 Validation Accuracy at 0.551800012588501\n",
      "Epoch 183, CIFAR-10 Batch 3:  Loss at 1.3696534633636475 Validation Accuracy at 0.5580000281333923\n",
      "Epoch 183, CIFAR-10 Batch 4:  Loss at 1.3622841835021973 Validation Accuracy at 0.5580000281333923\n",
      "Epoch 183, CIFAR-10 Batch 5:  Loss at 1.3559212684631348 Validation Accuracy at 0.5562000274658203\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss at 1.368263602256775 Validation Accuracy at 0.5577999949455261\n",
      "Epoch 184, CIFAR-10 Batch 2:  Loss at 1.3558897972106934 Validation Accuracy at 0.5527999997138977\n",
      "Epoch 184, CIFAR-10 Batch 3:  Loss at 1.3566638231277466 Validation Accuracy at 0.5613999962806702\n",
      "Epoch 184, CIFAR-10 Batch 4:  Loss at 1.3601610660552979 Validation Accuracy at 0.5619999766349792\n",
      "Epoch 184, CIFAR-10 Batch 5:  Loss at 1.3567842245101929 Validation Accuracy at 0.555400013923645\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss at 1.3708763122558594 Validation Accuracy at 0.5568000078201294\n",
      "Epoch 185, CIFAR-10 Batch 2:  Loss at 1.3697302341461182 Validation Accuracy at 0.551800012588501\n",
      "Epoch 185, CIFAR-10 Batch 3:  Loss at 1.3578273057937622 Validation Accuracy at 0.5595999956130981\n",
      "Epoch 185, CIFAR-10 Batch 4:  Loss at 1.3653035163879395 Validation Accuracy at 0.5616000294685364\n",
      "Epoch 185, CIFAR-10 Batch 5:  Loss at 1.369126796722412 Validation Accuracy at 0.5519999861717224\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss at 1.3652236461639404 Validation Accuracy at 0.5604000091552734\n",
      "Epoch 186, CIFAR-10 Batch 2:  Loss at 1.350525975227356 Validation Accuracy at 0.5569999814033508\n",
      "Epoch 186, CIFAR-10 Batch 3:  Loss at 1.3612370491027832 Validation Accuracy at 0.5627999901771545\n",
      "Epoch 186, CIFAR-10 Batch 4:  Loss at 1.3626962900161743 Validation Accuracy at 0.557200014591217\n",
      "Epoch 186, CIFAR-10 Batch 5:  Loss at 1.3619509935379028 Validation Accuracy at 0.5546000003814697\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss at 1.3646371364593506 Validation Accuracy at 0.5573999881744385\n",
      "Epoch 187, CIFAR-10 Batch 2:  Loss at 1.3660531044006348 Validation Accuracy at 0.5519999861717224\n",
      "Epoch 187, CIFAR-10 Batch 3:  Loss at 1.364864468574524 Validation Accuracy at 0.5619999766349792\n",
      "Epoch 187, CIFAR-10 Batch 4:  Loss at 1.3787665367126465 Validation Accuracy at 0.5559999942779541\n",
      "Epoch 187, CIFAR-10 Batch 5:  Loss at 1.3654228448867798 Validation Accuracy at 0.5550000071525574\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss at 1.357459545135498 Validation Accuracy at 0.557200014591217\n",
      "Epoch 188, CIFAR-10 Batch 2:  Loss at 1.3576757907867432 Validation Accuracy at 0.5580000281333923\n",
      "Epoch 188, CIFAR-10 Batch 3:  Loss at 1.3650439977645874 Validation Accuracy at 0.5509999990463257\n",
      "Epoch 188, CIFAR-10 Batch 4:  Loss at 1.3733386993408203 Validation Accuracy at 0.5587999820709229\n",
      "Epoch 188, CIFAR-10 Batch 5:  Loss at 1.3618417978286743 Validation Accuracy at 0.5616000294685364\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss at 1.3639060258865356 Validation Accuracy at 0.551800012588501\n",
      "Epoch 189, CIFAR-10 Batch 2:  Loss at 1.360371708869934 Validation Accuracy at 0.5511999726295471\n",
      "Epoch 189, CIFAR-10 Batch 3:  Loss at 1.3873997926712036 Validation Accuracy at 0.553600013256073\n",
      "Epoch 189, CIFAR-10 Batch 4:  Loss at 1.3770928382873535 Validation Accuracy at 0.5508000254631042\n",
      "Epoch 189, CIFAR-10 Batch 5:  Loss at 1.3624348640441895 Validation Accuracy at 0.5533999800682068\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss at 1.3630034923553467 Validation Accuracy at 0.5544000267982483\n",
      "Epoch 190, CIFAR-10 Batch 2:  Loss at 1.3545892238616943 Validation Accuracy at 0.5580000281333923\n",
      "Epoch 190, CIFAR-10 Batch 3:  Loss at 1.3743542432785034 Validation Accuracy at 0.5583999752998352\n",
      "Epoch 190, CIFAR-10 Batch 4:  Loss at 1.3723658323287964 Validation Accuracy at 0.5586000084877014\n",
      "Epoch 190, CIFAR-10 Batch 5:  Loss at 1.3663769960403442 Validation Accuracy at 0.5586000084877014\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss at 1.3695392608642578 Validation Accuracy at 0.5565999746322632\n",
      "Epoch 191, CIFAR-10 Batch 2:  Loss at 1.3649994134902954 Validation Accuracy at 0.5564000010490417\n",
      "Epoch 191, CIFAR-10 Batch 3:  Loss at 1.3801201581954956 Validation Accuracy at 0.5514000058174133\n",
      "Epoch 191, CIFAR-10 Batch 4:  Loss at 1.3717583417892456 Validation Accuracy at 0.5577999949455261\n",
      "Epoch 191, CIFAR-10 Batch 5:  Loss at 1.3629790544509888 Validation Accuracy at 0.5568000078201294\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss at 1.3653358221054077 Validation Accuracy at 0.5583999752998352\n",
      "Epoch 192, CIFAR-10 Batch 2:  Loss at 1.3576321601867676 Validation Accuracy at 0.5564000010490417\n",
      "Epoch 192, CIFAR-10 Batch 3:  Loss at 1.3845380544662476 Validation Accuracy at 0.557200014591217\n",
      "Epoch 192, CIFAR-10 Batch 4:  Loss at 1.3782039880752563 Validation Accuracy at 0.5601999759674072\n",
      "Epoch 192, CIFAR-10 Batch 5:  Loss at 1.3675249814987183 Validation Accuracy at 0.5590000152587891\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss at 1.374902606010437 Validation Accuracy at 0.5576000213623047\n",
      "Epoch 193, CIFAR-10 Batch 2:  Loss at 1.3664859533309937 Validation Accuracy at 0.5526000261306763\n",
      "Epoch 193, CIFAR-10 Batch 3:  Loss at 1.3702434301376343 Validation Accuracy at 0.5544000267982483\n",
      "Epoch 193, CIFAR-10 Batch 4:  Loss at 1.3773459196090698 Validation Accuracy at 0.5569999814033508\n",
      "Epoch 193, CIFAR-10 Batch 5:  Loss at 1.3647202253341675 Validation Accuracy at 0.5568000078201294\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss at 1.3862115144729614 Validation Accuracy at 0.5541999936103821\n",
      "Epoch 194, CIFAR-10 Batch 2:  Loss at 1.3645451068878174 Validation Accuracy at 0.5493999719619751\n",
      "Epoch 194, CIFAR-10 Batch 3:  Loss at 1.389864206314087 Validation Accuracy at 0.5509999990463257\n",
      "Epoch 194, CIFAR-10 Batch 4:  Loss at 1.3832355737686157 Validation Accuracy at 0.550599992275238\n",
      "Epoch 194, CIFAR-10 Batch 5:  Loss at 1.3723126649856567 Validation Accuracy at 0.553600013256073\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss at 1.378542184829712 Validation Accuracy at 0.5586000084877014\n",
      "Epoch 195, CIFAR-10 Batch 2:  Loss at 1.3668558597564697 Validation Accuracy at 0.5537999868392944\n",
      "Epoch 195, CIFAR-10 Batch 3:  Loss at 1.382908582687378 Validation Accuracy at 0.5544000267982483\n",
      "Epoch 195, CIFAR-10 Batch 4:  Loss at 1.390688419342041 Validation Accuracy at 0.5551999807357788\n",
      "Epoch 195, CIFAR-10 Batch 5:  Loss at 1.3797485828399658 Validation Accuracy at 0.5529999732971191\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss at 1.369408369064331 Validation Accuracy at 0.5573999881744385\n",
      "Epoch 196, CIFAR-10 Batch 2:  Loss at 1.3676296472549438 Validation Accuracy at 0.555400013923645\n",
      "Epoch 196, CIFAR-10 Batch 3:  Loss at 1.3887091875076294 Validation Accuracy at 0.5559999942779541\n",
      "Epoch 196, CIFAR-10 Batch 4:  Loss at 1.377694845199585 Validation Accuracy at 0.5540000200271606\n",
      "Epoch 196, CIFAR-10 Batch 5:  Loss at 1.3694701194763184 Validation Accuracy at 0.557200014591217\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss at 1.3684685230255127 Validation Accuracy at 0.5577999949455261\n",
      "Epoch 197, CIFAR-10 Batch 2:  Loss at 1.367264986038208 Validation Accuracy at 0.5482000112533569\n",
      "Epoch 197, CIFAR-10 Batch 3:  Loss at 1.3902181386947632 Validation Accuracy at 0.5586000084877014\n",
      "Epoch 197, CIFAR-10 Batch 4:  Loss at 1.3905727863311768 Validation Accuracy at 0.5450000166893005\n",
      "Epoch 197, CIFAR-10 Batch 5:  Loss at 1.371614694595337 Validation Accuracy at 0.5562000274658203\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss at 1.3764991760253906 Validation Accuracy at 0.5537999868392944\n",
      "Epoch 198, CIFAR-10 Batch 2:  Loss at 1.3867515325546265 Validation Accuracy at 0.5493999719619751\n",
      "Epoch 198, CIFAR-10 Batch 3:  Loss at 1.3961915969848633 Validation Accuracy at 0.553600013256073\n",
      "Epoch 198, CIFAR-10 Batch 4:  Loss at 1.3860465288162231 Validation Accuracy at 0.5519999861717224\n",
      "Epoch 198, CIFAR-10 Batch 5:  Loss at 1.3761210441589355 Validation Accuracy at 0.5546000003814697\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss at 1.3912116289138794 Validation Accuracy at 0.5537999868392944\n",
      "Epoch 199, CIFAR-10 Batch 2:  Loss at 1.3673548698425293 Validation Accuracy at 0.5515999794006348\n",
      "Epoch 199, CIFAR-10 Batch 3:  Loss at 1.3909178972244263 Validation Accuracy at 0.5546000003814697\n",
      "Epoch 199, CIFAR-10 Batch 4:  Loss at 1.3829710483551025 Validation Accuracy at 0.5547999739646912\n",
      "Epoch 199, CIFAR-10 Batch 5:  Loss at 1.3806085586547852 Validation Accuracy at 0.5522000193595886\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss at 1.3802417516708374 Validation Accuracy at 0.5537999868392944\n",
      "Epoch 200, CIFAR-10 Batch 2:  Loss at 1.3690240383148193 Validation Accuracy at 0.5522000193595886\n",
      "Epoch 200, CIFAR-10 Batch 3:  Loss at 1.3969298601150513 Validation Accuracy at 0.5537999868392944\n",
      "Epoch 200, CIFAR-10 Batch 4:  Loss at 1.3847451210021973 Validation Accuracy at 0.5546000003814697\n",
      "Epoch 200, CIFAR-10 Batch 5:  Loss at 1.3784195184707642 Validation Accuracy at 0.5497999787330627\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.54755859375\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxP1/bWin9erxs7cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7208a22828>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
