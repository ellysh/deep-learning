{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 4:\n",
      "Image - Min Value: 0 Max Value: 254\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcdc5a63630>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 4\n",
    "\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    return np.array(x / 255.)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return np.eye(10)[x]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=[None, image_shape[0], \n",
    "                                             image_shape[1], image_shape[2]],\n",
    "                          name=\"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=[None, n_classes],\n",
    "                          name=\"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement Function\n",
    "    input_depth = x_tensor.shape[3].value\n",
    "    filter_weights = tf.Variable(tf.truncated_normal((conv_ksize[0],\n",
    "                                                      conv_ksize[1], input_depth,\n",
    "                                                      conv_num_outputs))) # (height, width, input_depth, output_depth)\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    strides = [1, conv_strides[0], conv_strides[1], 1] # (batch, height, width, depth)\n",
    "    padding = 'VALID'\n",
    "\n",
    "    conv = tf.nn.conv2d(x_tensor, filter_weights, strides, padding) + bias\n",
    "    conv = tf.nn.bias_add(conv, bias)\n",
    "    conv = tf.nn.relu(conv)\n",
    "\n",
    "    conv = tf.nn.max_pool(conv, ksize=[1, pool_ksize[0], pool_ksize[1], 1],\n",
    "                          strides=[1, pool_strides[0], pool_ksize[1], 1],\n",
    "                          padding='VALID')\n",
    "\n",
    "    return conv\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    _, a, b, c = x_tensor.get_shape().as_list()\n",
    "    return tf.reshape(x_tensor, [-1, a * b * c])\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    weight = tf.Variable(tf.random_normal([x_tensor.shape[1].value,\n",
    "                                           num_outputs]))\n",
    "    bias = tf.Variable(tf.random_normal([num_outputs]))\n",
    "    \n",
    "    return tf.add(tf.matmul(x_tensor, weight), bias)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    conv = conv2d_maxpool(x, 10, (2,2), (4,4), (2,2), (2,2))\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    conv = flatten(conv)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    conv = fully_conn(conv, 40)\n",
    "    \n",
    "    conv = tf.nn.dropout(conv, keep_prob)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    conv = output(conv, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return conv\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer,\n",
    "                feed_dict={keep_prob: keep_probability,\n",
    "                           x: feature_batch, y: label_batch})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # Calculate Training and Validation accuracy\n",
    "    val_accuracy = session.run(accuracy, feed_dict={\n",
    "           x: valid_features,\n",
    "           y: valid_labels,\n",
    "           keep_prob: 1.0})\n",
    "\n",
    "    val_loss = session.run(cost, feed_dict={\n",
    "           x: valid_features,\n",
    "           y: valid_labels,\n",
    "           keep_prob: 1.0})\n",
    "\n",
    "    print('Loss at {}'.format(val_loss), 'Validation Accuracy at {}'.format(val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "keep_probability = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss at 2.389261245727539 Validation Accuracy at 0.10580000281333923\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss at 2.3501224517822266 Validation Accuracy at 0.11060000211000443\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss at 2.3067827224731445 Validation Accuracy at 0.13920000195503235\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss at 2.2890474796295166 Validation Accuracy at 0.14020000398159027\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss at 2.197288751602173 Validation Accuracy at 0.19519999623298645\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss at 2.1567506790161133 Validation Accuracy at 0.19779999554157257\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss at 2.115293264389038 Validation Accuracy at 0.2231999933719635\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss at 2.085634469985962 Validation Accuracy at 0.22699999809265137\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss at 2.0692849159240723 Validation Accuracy at 0.24300000071525574\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss at 2.055887460708618 Validation Accuracy at 0.25679999589920044\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss at 2.034417152404785 Validation Accuracy at 0.25760000944137573\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss at 2.0002784729003906 Validation Accuracy at 0.26080000400543213\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss at 1.9803274869918823 Validation Accuracy at 0.2939999997615814\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss at 1.9608925580978394 Validation Accuracy at 0.28060001134872437\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss at 1.9364218711853027 Validation Accuracy at 0.29600000381469727\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss at 1.9337652921676636 Validation Accuracy at 0.29280000925064087\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss at 1.937043309211731 Validation Accuracy at 0.29899999499320984\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss at 1.92564058303833 Validation Accuracy at 0.29760000109672546\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss at 1.927524447441101 Validation Accuracy at 0.3009999990463257\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss at 1.9062989950180054 Validation Accuracy at 0.30399999022483826\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss at 1.8896291255950928 Validation Accuracy at 0.32120001316070557\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss at 1.8977457284927368 Validation Accuracy at 0.31940001249313354\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss at 1.8952709436416626 Validation Accuracy at 0.3224000036716461\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss at 1.8837422132492065 Validation Accuracy at 0.3248000144958496\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss at 1.868932843208313 Validation Accuracy at 0.3264000117778778\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss at 1.8608275651931763 Validation Accuracy at 0.3357999920845032\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss at 1.868204116821289 Validation Accuracy at 0.335999995470047\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss at 1.882668375968933 Validation Accuracy at 0.32839998602867126\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss at 1.8435159921646118 Validation Accuracy at 0.3463999927043915\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss at 1.8534315824508667 Validation Accuracy at 0.3346000015735626\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss at 1.8564331531524658 Validation Accuracy at 0.3452000021934509\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss at 1.8355844020843506 Validation Accuracy at 0.3483999967575073\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss at 1.8434062004089355 Validation Accuracy at 0.34540000557899475\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss at 1.8309540748596191 Validation Accuracy at 0.3402000069618225\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss at 1.8217130899429321 Validation Accuracy at 0.34619998931884766\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss at 1.8422638177871704 Validation Accuracy at 0.33980000019073486\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss at 1.8303651809692383 Validation Accuracy at 0.35120001435279846\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss at 1.814677119255066 Validation Accuracy at 0.3582000136375427\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss at 1.8141992092132568 Validation Accuracy at 0.35359999537467957\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss at 1.7951253652572632 Validation Accuracy at 0.3628000020980835\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss at 1.7832324504852295 Validation Accuracy at 0.365200012922287\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss at 1.7904855012893677 Validation Accuracy at 0.3643999993801117\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss at 1.7719323635101318 Validation Accuracy at 0.37040001153945923\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss at 1.7738707065582275 Validation Accuracy at 0.37700000405311584\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss at 1.762471318244934 Validation Accuracy at 0.3776000142097473\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss at 1.7528265714645386 Validation Accuracy at 0.3824000060558319\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss at 1.7608355283737183 Validation Accuracy at 0.3874000012874603\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss at 1.7567652463912964 Validation Accuracy at 0.37599998712539673\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss at 1.7383042573928833 Validation Accuracy at 0.3862000107765198\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss at 1.7376987934112549 Validation Accuracy at 0.3901999890804291\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss at 1.732089877128601 Validation Accuracy at 0.38420000672340393\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss at 1.725088119506836 Validation Accuracy at 0.382999986410141\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss at 1.712982416152954 Validation Accuracy at 0.39239999651908875\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss at 1.7146726846694946 Validation Accuracy at 0.3869999945163727\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss at 1.7182550430297852 Validation Accuracy at 0.3901999890804291\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss at 1.708638072013855 Validation Accuracy at 0.39259999990463257\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss at 1.7094312906265259 Validation Accuracy at 0.3935999870300293\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss at 1.7032825946807861 Validation Accuracy at 0.3903999924659729\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss at 1.6999784708023071 Validation Accuracy at 0.39640000462532043\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss at 1.7043648958206177 Validation Accuracy at 0.39100000262260437\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss at 1.697307825088501 Validation Accuracy at 0.39800000190734863\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss at 1.693941593170166 Validation Accuracy at 0.3937999904155731\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss at 1.6971232891082764 Validation Accuracy at 0.3953999876976013\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss at 1.7029597759246826 Validation Accuracy at 0.3952000141143799\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss at 1.6926223039627075 Validation Accuracy at 0.39500001072883606\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss at 1.6804152727127075 Validation Accuracy at 0.4023999869823456\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss at 1.693263292312622 Validation Accuracy at 0.3986000120639801\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss at 1.6866543292999268 Validation Accuracy at 0.4004000127315521\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss at 1.680525541305542 Validation Accuracy at 0.4004000127315521\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss at 1.6715033054351807 Validation Accuracy at 0.40540000796318054\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss at 1.6741702556610107 Validation Accuracy at 0.40299999713897705\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss at 1.6646504402160645 Validation Accuracy at 0.4034000039100647\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss at 1.6719945669174194 Validation Accuracy at 0.4068000018596649\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss at 1.6631996631622314 Validation Accuracy at 0.40059998631477356\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss at 1.6660529375076294 Validation Accuracy at 0.4047999978065491\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss at 1.661988615989685 Validation Accuracy at 0.40880000591278076\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss at 1.6794078350067139 Validation Accuracy at 0.4023999869823456\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss at 1.6609296798706055 Validation Accuracy at 0.4034000039100647\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss at 1.668692946434021 Validation Accuracy at 0.4058000147342682\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss at 1.6679105758666992 Validation Accuracy at 0.4036000072956085\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss at 1.6606378555297852 Validation Accuracy at 0.4043999910354614\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss at 1.6691203117370605 Validation Accuracy at 0.40540000796318054\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss at 1.6675190925598145 Validation Accuracy at 0.4068000018596649\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss at 1.6644387245178223 Validation Accuracy at 0.4092000126838684\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss at 1.6459707021713257 Validation Accuracy at 0.4163999855518341\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss at 1.6610909700393677 Validation Accuracy at 0.4020000100135803\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss at 1.657788634300232 Validation Accuracy at 0.41100001335144043\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss at 1.6553945541381836 Validation Accuracy at 0.4097999930381775\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss at 1.6597270965576172 Validation Accuracy at 0.414000004529953\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss at 1.662183165550232 Validation Accuracy at 0.40860000252723694\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss at 1.6703909635543823 Validation Accuracy at 0.4075999855995178\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss at 1.6377513408660889 Validation Accuracy at 0.4153999984264374\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss at 1.646713137626648 Validation Accuracy at 0.4163999855518341\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss at 1.6401623487472534 Validation Accuracy at 0.4156000018119812\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss at 1.641369342803955 Validation Accuracy at 0.41819998621940613\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss at 1.6400537490844727 Validation Accuracy at 0.42179998755455017\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss at 1.6436419486999512 Validation Accuracy at 0.41519999504089355\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss at 1.6391136646270752 Validation Accuracy at 0.4221999943256378\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss at 1.6349676847457886 Validation Accuracy at 0.42080000042915344\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss at 1.627990961074829 Validation Accuracy at 0.42419999837875366\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss at 2.4158987998962402 Validation Accuracy at 0.13099999725818634\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss at 2.2631466388702393 Validation Accuracy at 0.18780000507831573\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss at 2.173584461212158 Validation Accuracy at 0.21439999341964722\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss at 2.140010356903076 Validation Accuracy at 0.2152000069618225\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss at 2.1016616821289062 Validation Accuracy at 0.24040000140666962\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss at 2.0479657649993896 Validation Accuracy at 0.26440000534057617\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss at 2.010406970977783 Validation Accuracy at 0.27300000190734863\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss at 1.9872334003448486 Validation Accuracy at 0.287200003862381\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss at 1.966844081878662 Validation Accuracy at 0.28679999709129333\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss at 1.9337433576583862 Validation Accuracy at 0.3093999922275543\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss at 1.9183976650238037 Validation Accuracy at 0.31439998745918274\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss at 1.9028871059417725 Validation Accuracy at 0.3131999969482422\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss at 1.855190396308899 Validation Accuracy at 0.33340001106262207\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss at 1.8301711082458496 Validation Accuracy at 0.34360000491142273\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss at 1.8318142890930176 Validation Accuracy at 0.34700000286102295\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss at 1.8085554838180542 Validation Accuracy at 0.35359999537467957\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss at 1.800492525100708 Validation Accuracy at 0.3587999939918518\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss at 1.7924437522888184 Validation Accuracy at 0.3569999933242798\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss at 1.7708687782287598 Validation Accuracy at 0.36660000681877136\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss at 1.7744213342666626 Validation Accuracy at 0.3659999966621399\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss at 1.750267744064331 Validation Accuracy at 0.3758000135421753\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss at 1.7412577867507935 Validation Accuracy at 0.3831999897956848\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss at 1.751220703125 Validation Accuracy at 0.3686000108718872\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss at 1.7364293336868286 Validation Accuracy at 0.37700000405311584\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss at 1.7384179830551147 Validation Accuracy at 0.37540000677108765\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss at 1.7135140895843506 Validation Accuracy at 0.37940001487731934\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss at 1.7248327732086182 Validation Accuracy at 0.38199999928474426\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss at 1.7433078289031982 Validation Accuracy at 0.3686000108718872\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss at 1.7117642164230347 Validation Accuracy at 0.3946000039577484\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss at 1.719090223312378 Validation Accuracy at 0.37720000743865967\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss at 1.6872020959854126 Validation Accuracy at 0.40220001339912415\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss at 1.70405912399292 Validation Accuracy at 0.39340001344680786\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss at 1.7006444931030273 Validation Accuracy at 0.3986000120639801\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss at 1.6954777240753174 Validation Accuracy at 0.3962000012397766\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss at 1.6865185499191284 Validation Accuracy at 0.3968000113964081\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss at 1.6759073734283447 Validation Accuracy at 0.41280001401901245\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss at 1.674241065979004 Validation Accuracy at 0.4047999978065491\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss at 1.681134581565857 Validation Accuracy at 0.4020000100135803\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss at 1.6587942838668823 Validation Accuracy at 0.41100001335144043\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss at 1.661359190940857 Validation Accuracy at 0.4124000072479248\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss at 1.6467245817184448 Validation Accuracy at 0.4129999876022339\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss at 1.6440867185592651 Validation Accuracy at 0.4198000133037567\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss at 1.6707794666290283 Validation Accuracy at 0.41359999775886536\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss at 1.6328742504119873 Validation Accuracy at 0.4214000105857849\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss at 1.650550365447998 Validation Accuracy at 0.41440001130104065\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss at 1.6260607242584229 Validation Accuracy at 0.4262000024318695\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss at 1.6280781030654907 Validation Accuracy at 0.4221999943256378\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss at 1.6448118686676025 Validation Accuracy at 0.4214000105857849\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss at 1.6206164360046387 Validation Accuracy at 0.4309999942779541\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss at 1.610658049583435 Validation Accuracy at 0.42739999294281006\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss at 1.6081193685531616 Validation Accuracy at 0.4374000132083893\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss at 1.6152217388153076 Validation Accuracy at 0.4293999969959259\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss at 1.6093716621398926 Validation Accuracy at 0.42579999566078186\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss at 1.6007994413375854 Validation Accuracy at 0.4327999949455261\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss at 1.6100560426712036 Validation Accuracy at 0.423799991607666\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss at 1.6010453701019287 Validation Accuracy at 0.43939998745918274\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss at 1.59481680393219 Validation Accuracy at 0.43700000643730164\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss at 1.6003533601760864 Validation Accuracy at 0.4325999915599823\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss at 1.578054666519165 Validation Accuracy at 0.4357999861240387\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss at 1.6005252599716187 Validation Accuracy at 0.4311999976634979\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss at 1.5917757749557495 Validation Accuracy at 0.4410000145435333\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss at 1.5799016952514648 Validation Accuracy at 0.4392000138759613\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss at 1.5881057977676392 Validation Accuracy at 0.43779999017715454\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss at 1.5721466541290283 Validation Accuracy at 0.44620001316070557\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss at 1.5821655988693237 Validation Accuracy at 0.42980000376701355\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss at 1.5644055604934692 Validation Accuracy at 0.448199987411499\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss at 1.5745809078216553 Validation Accuracy at 0.43959999084472656\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss at 1.5744397640228271 Validation Accuracy at 0.44519999623298645\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss at 1.562913417816162 Validation Accuracy at 0.4410000145435333\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss at 1.5834251642227173 Validation Accuracy at 0.4325999915599823\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss at 1.5698868036270142 Validation Accuracy at 0.44940000772476196\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss at 1.5674657821655273 Validation Accuracy at 0.446399986743927\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss at 1.5607843399047852 Validation Accuracy at 0.44760000705718994\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss at 1.5614882707595825 Validation Accuracy at 0.44620001316070557\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss at 1.5701379776000977 Validation Accuracy at 0.4453999996185303\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss at 1.557235836982727 Validation Accuracy at 0.4514000117778778\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss at 1.559129238128662 Validation Accuracy at 0.45159998536109924\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss at 1.5487756729125977 Validation Accuracy at 0.454800009727478\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss at 1.5454607009887695 Validation Accuracy at 0.44920000433921814\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss at 1.5599459409713745 Validation Accuracy at 0.4449999928474426\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss at 1.557194709777832 Validation Accuracy at 0.4496000111103058\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss at 1.5475759506225586 Validation Accuracy at 0.45260000228881836\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss at 1.5379095077514648 Validation Accuracy at 0.45820000767707825\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss at 1.552950382232666 Validation Accuracy at 0.44940000772476196\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss at 1.5523382425308228 Validation Accuracy at 0.4406000077724457\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss at 1.5381273031234741 Validation Accuracy at 0.45820000767707825\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss at 1.5442941188812256 Validation Accuracy at 0.4496000111103058\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss at 1.5331881046295166 Validation Accuracy at 0.4586000144481659\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss at 1.54023015499115 Validation Accuracy at 0.45399999618530273\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss at 1.546488642692566 Validation Accuracy at 0.4465999901294708\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss at 1.5340068340301514 Validation Accuracy at 0.45719999074935913\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss at 1.5328987836837769 Validation Accuracy at 0.4528000056743622\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss at 1.5255389213562012 Validation Accuracy at 0.4544000029563904\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss at 1.5407366752624512 Validation Accuracy at 0.45840001106262207\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss at 1.5343263149261475 Validation Accuracy at 0.4553999900817871\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss at 1.530606746673584 Validation Accuracy at 0.4652000069618225\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss at 1.5312612056732178 Validation Accuracy at 0.45980000495910645\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss at 1.5585520267486572 Validation Accuracy at 0.44600000977516174\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss at 1.529043197631836 Validation Accuracy at 0.45980000495910645\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss at 1.529746413230896 Validation Accuracy at 0.4560000002384186\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss at 1.5326156616210938 Validation Accuracy at 0.46059998869895935\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss at 1.535152792930603 Validation Accuracy at 0.4553999900817871\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss at 1.5153765678405762 Validation Accuracy at 0.4575999975204468\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss at 1.5399770736694336 Validation Accuracy at 0.46140000224113464\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss at 1.5204789638519287 Validation Accuracy at 0.46059998869895935\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss at 1.5155060291290283 Validation Accuracy at 0.4684000015258789\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss at 1.5154272317886353 Validation Accuracy at 0.460999995470047\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss at 1.519814372062683 Validation Accuracy at 0.4537999927997589\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss at 1.5102838277816772 Validation Accuracy at 0.46959999203681946\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss at 1.5170636177062988 Validation Accuracy at 0.4586000144481659\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss at 1.5177797079086304 Validation Accuracy at 0.46480000019073486\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss at 1.5177874565124512 Validation Accuracy at 0.4684000015258789\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss at 1.5036779642105103 Validation Accuracy at 0.4668000042438507\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss at 1.5180537700653076 Validation Accuracy at 0.46299999952316284\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss at 1.518557071685791 Validation Accuracy at 0.4625999927520752\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss at 1.5034714937210083 Validation Accuracy at 0.4650000035762787\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss at 1.508816123008728 Validation Accuracy at 0.46560001373291016\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss at 1.5116065740585327 Validation Accuracy at 0.459199994802475\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss at 1.5094884634017944 Validation Accuracy at 0.46720001101493835\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss at 1.5046253204345703 Validation Accuracy at 0.46299999952316284\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss at 1.5060659646987915 Validation Accuracy at 0.4742000102996826\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss at 1.4960179328918457 Validation Accuracy at 0.4659999907016754\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss at 1.5063722133636475 Validation Accuracy at 0.4668000042438507\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss at 1.5131709575653076 Validation Accuracy at 0.47119998931884766\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss at 1.504632830619812 Validation Accuracy at 0.46059998869895935\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss at 1.504774808883667 Validation Accuracy at 0.4691999852657318\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss at 1.5061737298965454 Validation Accuracy at 0.4650000035762787\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss at 1.4957401752471924 Validation Accuracy at 0.4643999934196472\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss at 1.5016415119171143 Validation Accuracy at 0.46700000762939453\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss at 1.497003197669983 Validation Accuracy at 0.4641999900341034\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss at 1.5026772022247314 Validation Accuracy at 0.4715999960899353\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss at 1.4999603033065796 Validation Accuracy at 0.46799999475479126\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss at 1.5051937103271484 Validation Accuracy at 0.46299999952316284\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss at 1.5017250776290894 Validation Accuracy at 0.46619999408721924\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss at 1.5014402866363525 Validation Accuracy at 0.4659999907016754\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss at 1.4931641817092896 Validation Accuracy at 0.4697999954223633\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss at 1.4877324104309082 Validation Accuracy at 0.4659999907016754\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss at 1.4914121627807617 Validation Accuracy at 0.46720001101493835\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss at 1.494886875152588 Validation Accuracy at 0.4697999954223633\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss at 1.4925836324691772 Validation Accuracy at 0.4740000069141388\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss at 1.4919382333755493 Validation Accuracy at 0.46959999203681946\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss at 1.4873994588851929 Validation Accuracy at 0.4690000116825104\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss at 1.498666524887085 Validation Accuracy at 0.4607999920845032\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss at 1.486582636833191 Validation Accuracy at 0.4715999960899353\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss at 1.5013766288757324 Validation Accuracy at 0.4684000015258789\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss at 1.4888927936553955 Validation Accuracy at 0.4729999899864197\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss at 1.4864224195480347 Validation Accuracy at 0.4690000116825104\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss at 1.4915807247161865 Validation Accuracy at 0.46399998664855957\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss at 1.4845818281173706 Validation Accuracy at 0.4742000102996826\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss at 1.4869201183319092 Validation Accuracy at 0.4674000144004822\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss at 1.4959701299667358 Validation Accuracy at 0.46860000491142273\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss at 1.4949780702590942 Validation Accuracy at 0.4659999907016754\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss at 1.489349603652954 Validation Accuracy at 0.4690000116825104\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss at 1.485703468322754 Validation Accuracy at 0.46939998865127563\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss at 1.4870859384536743 Validation Accuracy at 0.4702000021934509\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss at 1.4964115619659424 Validation Accuracy at 0.4690000116825104\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss at 1.4893571138381958 Validation Accuracy at 0.46540001034736633\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss at 1.4751529693603516 Validation Accuracy at 0.4706000089645386\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss at 1.4879872798919678 Validation Accuracy at 0.4708000123500824\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss at 1.483337640762329 Validation Accuracy at 0.47519999742507935\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss at 1.4754363298416138 Validation Accuracy at 0.4763999879360199\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss at 1.482637643814087 Validation Accuracy at 0.46720001101493835\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss at 1.475014090538025 Validation Accuracy at 0.4713999927043915\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss at 1.4794403314590454 Validation Accuracy at 0.47200000286102295\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss at 1.4786325693130493 Validation Accuracy at 0.47699999809265137\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss at 1.4833002090454102 Validation Accuracy at 0.4724000096321106\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss at 1.4808913469314575 Validation Accuracy at 0.4690000116825104\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss at 1.4702280759811401 Validation Accuracy at 0.4745999872684479\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss at 1.4815276861190796 Validation Accuracy at 0.4724000096321106\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss at 1.486621618270874 Validation Accuracy at 0.4666000008583069\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss at 1.4858309030532837 Validation Accuracy at 0.47540000081062317\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss at 1.4758710861206055 Validation Accuracy at 0.47200000286102295\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss at 1.4924973249435425 Validation Accuracy at 0.4652000069618225\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss at 1.486948013305664 Validation Accuracy at 0.4715999960899353\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss at 1.4755197763442993 Validation Accuracy at 0.4729999899864197\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss at 1.4789468050003052 Validation Accuracy at 0.47679999470710754\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss at 1.4726816415786743 Validation Accuracy at 0.46860000491142273\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss at 1.4712467193603516 Validation Accuracy at 0.4726000130176544\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss at 1.4752240180969238 Validation Accuracy at 0.4724000096321106\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss at 1.4810445308685303 Validation Accuracy at 0.4722000062465668\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss at 1.4953584671020508 Validation Accuracy at 0.47099998593330383\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss at 1.484258770942688 Validation Accuracy at 0.46399998664855957\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss at 1.4759652614593506 Validation Accuracy at 0.47119998931884766\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss at 1.4660323858261108 Validation Accuracy at 0.4747999906539917\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss at 1.4695775508880615 Validation Accuracy at 0.47699999809265137\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss at 1.4676589965820312 Validation Accuracy at 0.4758000075817108\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss at 1.474808931350708 Validation Accuracy at 0.4724000096321106\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss at 1.4689452648162842 Validation Accuracy at 0.4691999852657318\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss at 1.4760650396347046 Validation Accuracy at 0.47699999809265137\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss at 1.4642376899719238 Validation Accuracy at 0.48100000619888306\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss at 1.4723773002624512 Validation Accuracy at 0.47699999809265137\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss at 1.4633495807647705 Validation Accuracy at 0.47620001435279846\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss at 1.4624384641647339 Validation Accuracy at 0.47540000081062317\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss at 1.4645966291427612 Validation Accuracy at 0.47380000352859497\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss at 1.4686925411224365 Validation Accuracy at 0.4742000102996826\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss at 1.4654204845428467 Validation Accuracy at 0.475600004196167\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss at 1.4659348726272583 Validation Accuracy at 0.4787999987602234\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss at 1.4653406143188477 Validation Accuracy at 0.4733999967575073\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss at 1.470962643623352 Validation Accuracy at 0.4772000014781952\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss at 1.478285312652588 Validation Accuracy at 0.4684000015258789\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss at 1.4726808071136475 Validation Accuracy at 0.4758000075817108\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss at 1.4590835571289062 Validation Accuracy at 0.48019999265670776\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss at 1.4639142751693726 Validation Accuracy at 0.4733999967575073\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss at 1.469306230545044 Validation Accuracy at 0.47920000553131104\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss at 1.4550951719284058 Validation Accuracy at 0.4819999933242798\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss at 1.4609633684158325 Validation Accuracy at 0.48179998993873596\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss at 1.4608172178268433 Validation Accuracy at 0.4742000102996826\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss at 1.457013487815857 Validation Accuracy at 0.4747999906539917\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss at 1.4551653861999512 Validation Accuracy at 0.48100000619888306\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss at 1.4608054161071777 Validation Accuracy at 0.47999998927116394\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss at 1.4683908224105835 Validation Accuracy at 0.47679999470710754\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss at 1.4579273462295532 Validation Accuracy at 0.47760000824928284\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss at 1.45930814743042 Validation Accuracy at 0.4742000102996826\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss at 1.4534351825714111 Validation Accuracy at 0.4803999960422516\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss at 1.4574317932128906 Validation Accuracy at 0.4790000021457672\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss at 1.4558013677597046 Validation Accuracy at 0.4812000095844269\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss at 1.4550303220748901 Validation Accuracy at 0.477400004863739\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss at 1.4523448944091797 Validation Accuracy at 0.4763999879360199\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss at 1.4484783411026 Validation Accuracy at 0.4790000021457672\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss at 1.4497305154800415 Validation Accuracy at 0.483599990606308\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss at 1.4516037702560425 Validation Accuracy at 0.48179998993873596\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss at 1.4523824453353882 Validation Accuracy at 0.47859999537467957\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss at 1.451156497001648 Validation Accuracy at 0.47360000014305115\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss at 1.4515349864959717 Validation Accuracy at 0.4869999885559082\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss at 1.4416354894638062 Validation Accuracy at 0.4821999967098236\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss at 1.4599705934524536 Validation Accuracy at 0.4828000068664551\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss at 1.455906629562378 Validation Accuracy at 0.47920000553131104\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss at 1.4443626403808594 Validation Accuracy at 0.477400004863739\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss at 1.4481149911880493 Validation Accuracy at 0.48559999465942383\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss at 1.4526917934417725 Validation Accuracy at 0.4821999967098236\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss at 1.4661341905593872 Validation Accuracy at 0.4814000129699707\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss at 1.4436981678009033 Validation Accuracy at 0.4787999987602234\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss at 1.446215271949768 Validation Accuracy at 0.4797999858856201\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss at 1.4459166526794434 Validation Accuracy at 0.48840001225471497\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss at 1.445315957069397 Validation Accuracy at 0.4779999852180481\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss at 1.461210012435913 Validation Accuracy at 0.4790000021457672\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss at 1.4499056339263916 Validation Accuracy at 0.4790000021457672\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss at 1.442062258720398 Validation Accuracy at 0.47760000824928284\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss at 1.4397521018981934 Validation Accuracy at 0.4832000136375427\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss at 1.4517972469329834 Validation Accuracy at 0.47940000891685486\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss at 1.4559921026229858 Validation Accuracy at 0.48100000619888306\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss at 1.44527006149292 Validation Accuracy at 0.4819999933242798\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss at 1.437854290008545 Validation Accuracy at 0.4819999933242798\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss at 1.4391120672225952 Validation Accuracy at 0.4885999858379364\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss at 1.440492033958435 Validation Accuracy at 0.4796000123023987\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss at 1.447033405303955 Validation Accuracy at 0.48179998993873596\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss at 1.4409098625183105 Validation Accuracy at 0.4844000041484833\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss at 1.4377137422561646 Validation Accuracy at 0.4797999858856201\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss at 1.4407809972763062 Validation Accuracy at 0.4887999892234802\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss at 1.4416879415512085 Validation Accuracy at 0.4830000102519989\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss at 1.4494374990463257 Validation Accuracy at 0.48919999599456787\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss at 1.438443899154663 Validation Accuracy at 0.4821999967098236\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss at 1.4359948635101318 Validation Accuracy at 0.4805999994277954\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss at 1.4417283535003662 Validation Accuracy at 0.49000000953674316\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss at 1.4482342004776 Validation Accuracy at 0.4805999994277954\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss at 1.4401074647903442 Validation Accuracy at 0.4903999865055084\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss at 1.4357885122299194 Validation Accuracy at 0.4869999885559082\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss at 1.4255156517028809 Validation Accuracy at 0.4875999987125397\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss at 1.4312162399291992 Validation Accuracy at 0.4912000000476837\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss at 1.4318490028381348 Validation Accuracy at 0.4812000095844269\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss at 1.439595103263855 Validation Accuracy at 0.4878000020980835\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss at 1.4382520914077759 Validation Accuracy at 0.48159998655319214\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss at 1.4320179224014282 Validation Accuracy at 0.4837999939918518\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss at 1.439284324645996 Validation Accuracy at 0.48980000615119934\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss at 1.4365772008895874 Validation Accuracy at 0.47859999537467957\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss at 1.4490450620651245 Validation Accuracy at 0.48980000615119934\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss at 1.4319965839385986 Validation Accuracy at 0.49300000071525574\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss at 1.4341826438903809 Validation Accuracy at 0.4812000095844269\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss at 1.4336607456207275 Validation Accuracy at 0.4887999892234802\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss at 1.434072494506836 Validation Accuracy at 0.48980000615119934\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss at 1.4380862712860107 Validation Accuracy at 0.4896000027656555\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss at 1.4304349422454834 Validation Accuracy at 0.4903999865055084\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss at 1.4377471208572388 Validation Accuracy at 0.4869999885559082\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss at 1.4350788593292236 Validation Accuracy at 0.4846000075340271\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss at 1.4330769777297974 Validation Accuracy at 0.4790000021457672\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss at 1.4348981380462646 Validation Accuracy at 0.49219998717308044\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss at 1.4308582544326782 Validation Accuracy at 0.49059998989105225\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss at 1.4324283599853516 Validation Accuracy at 0.4864000082015991\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss at 1.4369148015975952 Validation Accuracy at 0.4918000102043152\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss at 1.4465227127075195 Validation Accuracy at 0.48820000886917114\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss at 1.448418140411377 Validation Accuracy at 0.4893999993801117\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss at 1.4231606721878052 Validation Accuracy at 0.4943999946117401\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss at 1.4307575225830078 Validation Accuracy at 0.4844000041484833\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss at 1.4372750520706177 Validation Accuracy at 0.49160000681877136\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss at 1.4322313070297241 Validation Accuracy at 0.4887999892234802\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss at 1.4323679208755493 Validation Accuracy at 0.4909999966621399\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss at 1.4369860887527466 Validation Accuracy at 0.49239999055862427\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss at 1.429160475730896 Validation Accuracy at 0.4887999892234802\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss at 1.4288389682769775 Validation Accuracy at 0.4909999966621399\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss at 1.4260294437408447 Validation Accuracy at 0.48919999599456787\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss at 1.4361445903778076 Validation Accuracy at 0.4875999987125397\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss at 1.4296003580093384 Validation Accuracy at 0.4912000000476837\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss at 1.425110101699829 Validation Accuracy at 0.4864000082015991\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss at 1.4337214231491089 Validation Accuracy at 0.49399998784065247\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss at 1.4255988597869873 Validation Accuracy at 0.4875999987125397\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss at 1.4330127239227295 Validation Accuracy at 0.4887999892234802\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss at 1.4278959035873413 Validation Accuracy at 0.4957999885082245\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss at 1.4301445484161377 Validation Accuracy at 0.48159998655319214\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss at 1.430601954460144 Validation Accuracy at 0.4936000108718872\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss at 1.434543490409851 Validation Accuracy at 0.4864000082015991\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss at 1.4267933368682861 Validation Accuracy at 0.4918000102043152\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss at 1.4252684116363525 Validation Accuracy at 0.4936000108718872\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss at 1.4203065633773804 Validation Accuracy at 0.4864000082015991\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss at 1.430666446685791 Validation Accuracy at 0.4934000074863434\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss at 1.4288357496261597 Validation Accuracy at 0.48539999127388\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss at 1.4258956909179688 Validation Accuracy at 0.49219998717308044\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss at 1.427873134613037 Validation Accuracy at 0.4941999912261963\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss at 1.4279063940048218 Validation Accuracy at 0.48919999599456787\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss at 1.4251408576965332 Validation Accuracy at 0.49160000681877136\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss at 1.4461339712142944 Validation Accuracy at 0.47760000824928284\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss at 1.428511619567871 Validation Accuracy at 0.48840001225471497\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss at 1.423459529876709 Validation Accuracy at 0.4968000054359436\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss at 1.42461097240448 Validation Accuracy at 0.48539999127388\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss at 1.4260034561157227 Validation Accuracy at 0.4950000047683716\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss at 1.4254909753799438 Validation Accuracy at 0.48899999260902405\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss at 1.4286566972732544 Validation Accuracy at 0.48840001225471497\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss at 1.4240366220474243 Validation Accuracy at 0.4878000020980835\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss at 1.4236176013946533 Validation Accuracy at 0.48919999599456787\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss at 1.4278488159179688 Validation Accuracy at 0.4934000074863434\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss at 1.4225115776062012 Validation Accuracy at 0.4875999987125397\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss at 1.4291688203811646 Validation Accuracy at 0.48399999737739563\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss at 1.4254447221755981 Validation Accuracy at 0.49720001220703125\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss at 1.4217240810394287 Validation Accuracy at 0.49399998784065247\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss at 1.43143630027771 Validation Accuracy at 0.5012000203132629\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss at 1.4241443872451782 Validation Accuracy at 0.4860000014305115\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss at 1.4301645755767822 Validation Accuracy at 0.4941999912261963\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss at 1.4239543676376343 Validation Accuracy at 0.4934000074863434\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss at 1.4172792434692383 Validation Accuracy at 0.492000013589859\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss at 1.417703628540039 Validation Accuracy at 0.4970000088214874\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss at 1.4234976768493652 Validation Accuracy at 0.49079999327659607\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss at 1.4171127080917358 Validation Accuracy at 0.49480000138282776\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss at 1.4195393323898315 Validation Accuracy at 0.4943999946117401\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss at 1.4159493446350098 Validation Accuracy at 0.48980000615119934\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss at 1.4193074703216553 Validation Accuracy at 0.48840001225471497\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss at 1.429013729095459 Validation Accuracy at 0.4887999892234802\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss at 1.4182994365692139 Validation Accuracy at 0.49399998784065247\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss at 1.4233649969100952 Validation Accuracy at 0.4918000102043152\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss at 1.4239685535430908 Validation Accuracy at 0.48739999532699585\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss at 1.4129204750061035 Validation Accuracy at 0.49939998984336853\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss at 1.4165798425674438 Validation Accuracy at 0.49320000410079956\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss at 1.4170916080474854 Validation Accuracy at 0.49320000410079956\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss at 1.4164561033248901 Validation Accuracy at 0.48899999260902405\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss at 1.4134564399719238 Validation Accuracy at 0.49459999799728394\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss at 1.4161630868911743 Validation Accuracy at 0.49779999256134033\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss at 1.4134482145309448 Validation Accuracy at 0.49380001425743103\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss at 1.4144185781478882 Validation Accuracy at 0.4991999864578247\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss at 1.4151517152786255 Validation Accuracy at 0.4970000088214874\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss at 1.408036708831787 Validation Accuracy at 0.49900001287460327\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss at 1.4096137285232544 Validation Accuracy at 0.49880000948905945\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss at 1.424041509628296 Validation Accuracy at 0.4936000108718872\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss at 1.4114785194396973 Validation Accuracy at 0.5009999871253967\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss at 1.4063141345977783 Validation Accuracy at 0.49540001153945923\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss at 1.4099873304367065 Validation Accuracy at 0.4952000081539154\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss at 1.4109587669372559 Validation Accuracy at 0.5037999749183655\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss at 1.41188645362854 Validation Accuracy at 0.49380001425743103\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss at 1.4122601747512817 Validation Accuracy at 0.4941999912261963\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss at 1.4137208461761475 Validation Accuracy at 0.5008000135421753\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss at 1.404890775680542 Validation Accuracy at 0.49380001425743103\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss at 1.4073987007141113 Validation Accuracy at 0.5008000135421753\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss at 1.411996841430664 Validation Accuracy at 0.49000000953674316\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss at 1.4165598154067993 Validation Accuracy at 0.4936000108718872\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss at 1.411645531654358 Validation Accuracy at 0.49900001287460327\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss at 1.4038689136505127 Validation Accuracy at 0.49399998784065247\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss at 1.4079514741897583 Validation Accuracy at 0.4997999966144562\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss at 1.418735146522522 Validation Accuracy at 0.49079999327659607\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss at 1.418121337890625 Validation Accuracy at 0.498199999332428\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss at 1.4021819829940796 Validation Accuracy at 0.501800000667572\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss at 1.4057416915893555 Validation Accuracy at 0.49239999055862427\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss at 1.403292179107666 Validation Accuracy at 0.5005999803543091\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss at 1.4101064205169678 Validation Accuracy at 0.49300000071525574\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss at 1.407321572303772 Validation Accuracy at 0.4959999918937683\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss at 1.3964345455169678 Validation Accuracy at 0.5009999871253967\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss at 1.401699185371399 Validation Accuracy at 0.49459999799728394\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss at 1.4140576124191284 Validation Accuracy at 0.49459999799728394\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss at 1.4093008041381836 Validation Accuracy at 0.49480000138282776\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss at 1.398136019706726 Validation Accuracy at 0.5019999742507935\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss at 1.4033448696136475 Validation Accuracy at 0.49779999256134033\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss at 1.3982384204864502 Validation Accuracy at 0.4957999885082245\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss at 1.4075901508331299 Validation Accuracy at 0.5016000270843506\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss at 1.402239441871643 Validation Accuracy at 0.4973999857902527\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss at 1.3993182182312012 Validation Accuracy at 0.5001999735832214\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss at 1.3988877534866333 Validation Accuracy at 0.5037999749183655\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss at 1.4051913022994995 Validation Accuracy at 0.49540001153945923\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss at 1.4028877019882202 Validation Accuracy at 0.5040000081062317\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss at 1.4080638885498047 Validation Accuracy at 0.4952000081539154\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss at 1.4134281873703003 Validation Accuracy at 0.4970000088214874\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss at 1.403182864189148 Validation Accuracy at 0.5026000142097473\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss at 1.4030917882919312 Validation Accuracy at 0.4984000027179718\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss at 1.4042928218841553 Validation Accuracy at 0.49779999256134033\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss at 1.406313419342041 Validation Accuracy at 0.498199999332428\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss at 1.4009703397750854 Validation Accuracy at 0.49799999594688416\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss at 1.401515007019043 Validation Accuracy at 0.5013999938964844\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss at 1.404219388961792 Validation Accuracy at 0.4934000074863434\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss at 1.3973722457885742 Validation Accuracy at 0.5004000067710876\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss at 1.3988085985183716 Validation Accuracy at 0.501800000667572\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss at 1.4004433155059814 Validation Accuracy at 0.5008000135421753\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss at 1.405148983001709 Validation Accuracy at 0.4966000020503998\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss at 1.3988786935806274 Validation Accuracy at 0.49939998984336853\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss at 1.400899052619934 Validation Accuracy at 0.5004000067710876\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss at 1.3965767621994019 Validation Accuracy at 0.5001999735832214\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss at 1.398870825767517 Validation Accuracy at 0.501800000667572\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss at 1.3990548849105835 Validation Accuracy at 0.49799999594688416\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss at 1.3984992504119873 Validation Accuracy at 0.5019999742507935\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss at 1.4003719091415405 Validation Accuracy at 0.5023999810218811\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss at 1.4132729768753052 Validation Accuracy at 0.4934000074863434\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss at 1.3999971151351929 Validation Accuracy at 0.4968000054359436\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss at 1.3974428176879883 Validation Accuracy at 0.503000020980835\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss at 1.392778754234314 Validation Accuracy at 0.5013999938964844\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss at 1.3965561389923096 Validation Accuracy at 0.5088000297546387\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss at 1.3934060335159302 Validation Accuracy at 0.5013999938964844\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss at 1.4002224206924438 Validation Accuracy at 0.5037999749183655\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss at 1.3946517705917358 Validation Accuracy at 0.5045999884605408\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss at 1.3977779150009155 Validation Accuracy at 0.49939998984336853\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss at 1.3896377086639404 Validation Accuracy at 0.5080000162124634\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss at 1.3914194107055664 Validation Accuracy at 0.501800000667572\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss at 1.3937058448791504 Validation Accuracy at 0.49959999322891235\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss at 1.393340826034546 Validation Accuracy at 0.5067999958992004\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss at 1.3934818506240845 Validation Accuracy at 0.503000020980835\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss at 1.3960767984390259 Validation Accuracy at 0.5076000094413757\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss at 1.393407940864563 Validation Accuracy at 0.5049999952316284\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss at 1.391978144645691 Validation Accuracy at 0.5031999945640564\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss at 1.4052379131317139 Validation Accuracy at 0.5059999823570251\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss at 1.3965891599655151 Validation Accuracy at 0.5001999735832214\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss at 1.3955310583114624 Validation Accuracy at 0.5052000284194946\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss at 1.3961687088012695 Validation Accuracy at 0.5013999938964844\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss at 1.4004876613616943 Validation Accuracy at 0.5016000270843506\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss at 1.3942816257476807 Validation Accuracy at 0.5058000087738037\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss at 1.396745204925537 Validation Accuracy at 0.5034000277519226\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss at 1.4010560512542725 Validation Accuracy at 0.5063999891281128\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss at 1.3970531225204468 Validation Accuracy at 0.503000020980835\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss at 1.3998539447784424 Validation Accuracy at 0.5013999938964844\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss at 1.3918805122375488 Validation Accuracy at 0.5081999897956848\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss at 1.392586350440979 Validation Accuracy at 0.5054000020027161\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss at 1.3896567821502686 Validation Accuracy at 0.5072000026702881\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss at 1.3950920104980469 Validation Accuracy at 0.5045999884605408\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss at 1.3919609785079956 Validation Accuracy at 0.5023999810218811\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss at 1.3913536071777344 Validation Accuracy at 0.504800021648407\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss at 1.3924107551574707 Validation Accuracy at 0.5009999871253967\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss at 1.3839384317398071 Validation Accuracy at 0.5045999884605408\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss at 1.391662836074829 Validation Accuracy at 0.5052000284194946\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss at 1.3992648124694824 Validation Accuracy at 0.501800000667572\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss at 1.3931585550308228 Validation Accuracy at 0.5055999755859375\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss at 1.3895423412322998 Validation Accuracy at 0.503000020980835\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss at 1.391417384147644 Validation Accuracy at 0.5008000135421753\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss at 1.392422080039978 Validation Accuracy at 0.5022000074386597\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss at 1.3991830348968506 Validation Accuracy at 0.4975999891757965\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss at 1.3952596187591553 Validation Accuracy at 0.5013999938964844\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss at 1.3872567415237427 Validation Accuracy at 0.504800021648407\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss at 1.3926818370819092 Validation Accuracy at 0.4997999966144562\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss at 1.391473412513733 Validation Accuracy at 0.5044000148773193\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss at 1.390523076057434 Validation Accuracy at 0.5027999877929688\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss at 1.4026199579238892 Validation Accuracy at 0.49900001287460327\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss at 1.3850727081298828 Validation Accuracy at 0.4986000061035156\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss at 1.3895198106765747 Validation Accuracy at 0.49959999322891235\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss at 1.3909740447998047 Validation Accuracy at 0.5023999810218811\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss at 1.3968613147735596 Validation Accuracy at 0.5005999803543091\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss at 1.39521324634552 Validation Accuracy at 0.5067999958992004\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss at 1.3889809846878052 Validation Accuracy at 0.4991999864578247\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss at 1.3922364711761475 Validation Accuracy at 0.503000020980835\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss at 1.3911653757095337 Validation Accuracy at 0.5026000142097473\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss at 1.395172357559204 Validation Accuracy at 0.4968000054359436\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss at 1.3908703327178955 Validation Accuracy at 0.5044000148773193\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss at 1.3853129148483276 Validation Accuracy at 0.503000020980835\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss at 1.3894290924072266 Validation Accuracy at 0.5012000203132629\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss at 1.382824182510376 Validation Accuracy at 0.508400022983551\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss at 1.3934381008148193 Validation Accuracy at 0.5004000067710876\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss at 1.3864259719848633 Validation Accuracy at 0.5049999952316284\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss at 1.3886200189590454 Validation Accuracy at 0.5040000081062317\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss at 1.390380620956421 Validation Accuracy at 0.5063999891281128\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss at 1.3874520063400269 Validation Accuracy at 0.5049999952316284\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss at 1.3977402448654175 Validation Accuracy at 0.49939998984336853\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss at 1.3929699659347534 Validation Accuracy at 0.5031999945640564\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss at 1.3858239650726318 Validation Accuracy at 0.5031999945640564\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss at 1.391973614692688 Validation Accuracy at 0.5040000081062317\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss at 1.3871861696243286 Validation Accuracy at 0.504800021648407\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss at 1.3959689140319824 Validation Accuracy at 0.5008000135421753\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss at 1.3941020965576172 Validation Accuracy at 0.5040000081062317\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss at 1.385041356086731 Validation Accuracy at 0.5077999830245972\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss at 1.3876293897628784 Validation Accuracy at 0.5022000074386597\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss at 1.38960599899292 Validation Accuracy at 0.5045999884605408\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss at 1.3923708200454712 Validation Accuracy at 0.501800000667572\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss at 1.3906058073043823 Validation Accuracy at 0.5016000270843506\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss at 1.3868122100830078 Validation Accuracy at 0.5027999877929688\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss at 1.3824964761734009 Validation Accuracy at 0.5073999762535095\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss at 1.3886216878890991 Validation Accuracy at 0.5034000277519226\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss at 1.3860076665878296 Validation Accuracy at 0.506600022315979\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss at 1.3967022895812988 Validation Accuracy at 0.5077999830245972\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss at 1.383317232131958 Validation Accuracy at 0.5070000290870667\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss at 1.3881839513778687 Validation Accuracy at 0.5081999897956848\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss at 1.3811556100845337 Validation Accuracy at 0.5059999823570251\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss at 1.3917323350906372 Validation Accuracy at 0.5004000067710876\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss at 1.3902415037155151 Validation Accuracy at 0.5072000026702881\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss at 1.3794971704483032 Validation Accuracy at 0.5080000162124634\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss at 1.392655372619629 Validation Accuracy at 0.5076000094413757\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss at 1.3824334144592285 Validation Accuracy at 0.5008000135421753\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss at 1.3891652822494507 Validation Accuracy at 0.5004000067710876\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss at 1.391513705253601 Validation Accuracy at 0.5031999945640564\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss at 1.3854806423187256 Validation Accuracy at 0.5112000107765198\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss at 1.3861172199249268 Validation Accuracy at 0.5094000101089478\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss at 1.3874685764312744 Validation Accuracy at 0.5016000270843506\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.5053742038216561\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcdb0ec2630>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
